{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Lab 04: Multinomial Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Generalized Linear Models\n",
    "\n",
    "From lecture, we know that members of the exponential family distributions can be written in the form\n",
    "$$p(y;\\eta) = b(y)e^{(\\eta^\\top T(y)-a(\\eta))},$$\n",
    "where\n",
    "- $\\eta$ is the natural parameter or canonical paramter of the distribution,\n",
    "- $T(y)$ is the sufficient statistic (we normally use $T(y) = y$),\n",
    "- $b(y)$ is an arbitrary scalar function of y, and\n",
    "- $a(\\eta)$ is the log partition function. We use $e^{a(\\eta)}$ just to normalize the distribution to have a sum or integral of 1.\n",
    "\n",
    "Each choice of $T$, $a$, and $b$ defines a family (set) of distributions parameterized by $\\eta$.\n",
    "\n",
    "If we can write $p(y \\mid \\mathbf{x} ; \\theta)$ as a member of the exponential family of distributions with parameters $\\mathbf{\\eta}$ with\n",
    "$\\eta_i = \\theta^\\top_i \\mathbf{x}$, we obtain a *generalized linear model* that can be optimized using the maximum likelihood principle.\n",
    "\n",
    "The GLM for the Gaussian distribution with natural parameter $\\eta$ being the mean of the Gaussian gives us ordinary linear regression.\n",
    "\n",
    "The Bernoulli distribution with parameter $\\phi$ can be written as an exponential distribution\n",
    "with natural parmeter $\\eta = \\log \\frac{\\phi}{1-\\phi}$. The GLM for this distribution is logistic regression.\n",
    "\n",
    "When we write the multinomial distribution with paremeters $\\phi_i > 0$ for classes $i \\in 1..k$ with the constraint that\n",
    "$$\\sum_{i=1}^{k} \\phi_i = 1$$ as a member of the exponential family,\n",
    "the resulting GLM is called *multinomial logistic regression*. The parameters $\\phi_1, \\ldots, \\phi_k$ are written\n",
    "in terms of $\\theta$ as\n",
    "$$\\phi_i = p(y = i \\mid \\mathbf{x}; \\theta) = \\frac{e^{\\theta^\\top_i \\mathbf{x}}}{\\sum_{j=1}^{k}e^{\\theta^\\top_j \\mathbf{x}}}. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Regression Example\n",
    "\n",
    "The following example of multinomial logistic regression is from [Kaggle](https://www.kaggle.com/saksham219/softmax-regression-for-iris-classification).\n",
    "\n",
    "The data set is the famous [Iris dataset from the UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/iris).\n",
    "\n",
    "The data contain 50 samples from each of three classes. Each class refers to a particular species of the iris plant. \n",
    "The data include four independent variables:\n",
    "1. Sepal length in cm\n",
    "2. Sepal width in cm\n",
    "3. Petal length in cm\n",
    "4. Petal width in cm\n",
    "\n",
    "The target takes on one of three classes:\n",
    "1. Iris Setosa\n",
    "2. Iris Versicolour\n",
    "3. Iris Virginica\n",
    "   \n",
    "To predict the target value, we use multinomial logistic regression for $k=3$ classes i.e. $y \\in \\{ 1, 2, 3 \\}$. \n",
    "\n",
    "Given $\\mathbf{x}$, we would like to predict a probability distribution over the\n",
    "three outcomes for $y$, i.e., $\\phi_1 = p(y=1 \\mid \\mathbf{x})$, $\\phi_2 = p(y=2 \\mid \\mathbf{x})$, and $\\phi_3 = p(y=3 \\mid \\mathbf{x})$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `phi` function returns $\\phi_i$ for input patterns $\\mathtt{X}$ and parameters $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(i, theta, X):\n",
    "    mat_theta = np.matrix(theta[i])\n",
    "    mat_x = np.matrix(X)\n",
    "    num = math.exp(np.dot(mat_theta,mat_x.T))\n",
    "    den = 0\n",
    "    for j in range(0,k):\n",
    "        mat_theta_j = np.matrix(theta[j])\n",
    "        den = den + math.exp(np.dot(mat_theta_j,mat_x.T))\n",
    "    phi_i = num/den\n",
    "    return phi_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grad_cost` function gives the gradient of the cost for data $\\mathtt{X}, \\mathbf{y}$ for class $j\\in 1..k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator(i, j):\n",
    "    if i == j: return 1\n",
    "    else: return 0\n",
    "\n",
    "def grad_cost(X, y, j, theta):\n",
    "    sum = np.array([0 for i in range(0,n)])\n",
    "    for i in range(0, m):\n",
    "        p = indicator(y[i], j) - phi(j, theta,X.loc[i])\n",
    "        sum = sum + (X.loc[i] * p)\n",
    "    grad = -sum/m\n",
    "    return grad\n",
    "\n",
    "def gradient_descent(X, y, theta, alpha, iters):\n",
    "    costt = np.zeros(iters)\n",
    "    for iter in range(iters):\n",
    "        for j in range(0, k):\n",
    "            theta[j] = theta[j] - alpha * grad_cost(X, y, j, theta)\n",
    "        costt[iter] = cost(X,y,j,theta)\n",
    "    return theta,costt\n",
    "\n",
    "def h(X, theta):\n",
    "    X = np.matrix(X)\n",
    "    h_matrix = np.empty((k,1))\n",
    "    den = 0\n",
    "    for j in range(0,k):\n",
    "        #print(np.exp(theta@X.T).shape)\n",
    "        den = den + math.exp(theta[j]@X.T)\n",
    "    for i in range(0,k):\n",
    "        #print(np.exp(theta[i]@X.T).shape)\n",
    "        h_matrix[i] = math.exp(np.dot(theta[i],X.T))\n",
    "    h_matrix = h_matrix/den\n",
    "    return h_matrix\n",
    "\n",
    "def cost(X,y,j,theta):\n",
    "    p = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        #print(\"h(X,theta)\",h(X,theta))\n",
    "        for k in range(theta.shape[0]):\n",
    "            p -= indicator(y[i], j)*np.log(phi(j, theta,X.loc[i]))\n",
    "        #j += (1-y)*np.log(1-h(X,theta))+y*log(h(X,theta))\n",
    "    print(p,end=\" \")\n",
    "    return p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
      "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
      "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
      "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
      "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
      "4   5            5.0           3.6            1.4           0.2  Iris-setosa\n",
      "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
      "0            5.1           3.5            1.4           0.2  Iris-setosa\n",
      "1            4.9           3.0            1.4           0.2  Iris-setosa\n",
      "2            4.7           3.2            1.3           0.2  Iris-setosa\n",
      "3            4.6           3.1            1.5           0.2  Iris-setosa\n",
      "4            5.0           3.6            1.4           0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Iris.csv')\n",
    "print(data.head())\n",
    "\n",
    "data = data.drop(['Id'],axis=1)\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "# extract y from data\n",
    "y_label = 'Species';\n",
    "\n",
    "y =data[y_label];\n",
    "\n",
    "y_index = data.columns.get_loc(y_label)\n",
    "# extract features from data\n",
    "X = data.iloc[:,:y_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X.shape\n",
    "idx = np.arange(0,m)\n",
    "\n",
    "# Partion data into training and testing dataset\n",
    "random.shuffle(idx)\n",
    "data = data.iloc[idx,:]\n",
    "data = data.reset_index()\n",
    "data = data.drop(['index'],axis=1)\n",
    "percent_train = 0.7\n",
    "m_train = int(m*percent_train)\n",
    "\n",
    "X_train = data.iloc[0:m_train,0:y_index];\n",
    "X_test = data.iloc[m_train:,0:y_index];\n",
    "\n",
    "y_train = data.iloc[:m_train,y_index];\n",
    "y_test = data.iloc[m_train:,y_index];\n",
    "labels = pd.unique(data[y_label])\n",
    "\n",
    "# Encode target labels as integers 0..k-1\n",
    "\n",
    "i = 0\n",
    "for label in y.unique():\n",
    "    y_train[y_train.str.match(label)] = str(i)\n",
    "    y_test[y_test.str.match(label)] = str(i)\n",
    "    i = i + 1      \n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.04028648346771 86.29833618442886 79.78760081417789 74.86563960159317 72.01200163451495 69.5961568483237 67.67479125251232 66.02908527825646 64.6097530429388 63.370954767477095 62.277325161200245 61.303098281801354 60.42757714180143 59.63438349226308 58.91041778451944 58.24510035573391 57.62981962307977 57.057514988649686 56.52235739418725 56.01950308977653 55.544902341657476 55.09514959022927 54.66736518954484 54.25910142883278 53.86826736584077 53.49306833508365 53.13195697991498 52.783593391464386 52.446812487803 52.12059718227212 51.80405620603887 51.49640569202969 51.196953813916586 50.905087918413386 50.62026370182154 50.34199607008723 50.06985139120186 49.803440903867426 49.54241509016917 49.286458855030304 49.03528738334137 48.78864256833385 48.54628992312898 48.30801590231526 48.07362557259325 47.84294058150223 47.615797381448814 47.39204567302608 47.17154703721752 46.95417373073233 \n",
      "[[1.14459274 1.26711356 1.65244106 0.02585582 0.54753307]\n",
      " [1.04730866 1.12266646 0.7777915  1.17136518 0.91023013]\n",
      " [0.8144691  0.63387923 0.5933535  1.79043722 1.53438668]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# k is the number of unique labels\n",
    "\n",
    "k = len(y.unique())\n",
    "\n",
    "if (X_train.shape[1] == X.shape[1]): \n",
    "    X_train.insert(0, \"intercept\", 1)\n",
    "\n",
    "# Reset m and n for training data\n",
    "\n",
    "m, n = X_train.shape\n",
    "\n",
    "# Initialize theta for each class  \n",
    "\n",
    "theta_initial = np.ones((k,n))\n",
    "\n",
    "alpha = .12\n",
    "iterations = 50\n",
    "\n",
    "# Logistic regression\n",
    "\n",
    "theta,costt = gradient_descent(X_train,y_train,theta_initial, alpha, iterations)\n",
    "\n",
    "print()\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe630907b80>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "iterationss = np.arange(1,iterations+1)\n",
    "plt.plot(iterationss,costt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9111\n"
     ]
    }
   ],
   "source": [
    "# Predicting classes on test data \n",
    "\n",
    "if (X_test.shape[1] == X.shape[1]): \n",
    "    X_test.insert(0, \"intercept\", 1)\n",
    "\n",
    "# Reset m and n for test data\n",
    "\n",
    "m,n = X_test.shape\n",
    "\n",
    "y_pred = []\n",
    "for index,row in X_test.iterrows():\n",
    "    h_matrix = h(row, theta)\n",
    "    prediction = int(np.where(h_matrix == h_matrix.max())[0])\n",
    "    y_pred.append(prediction)\n",
    "        \n",
    "# Estimate accuracy of model on test data        \n",
    "\n",
    "correct = (y_pred == y_test).value_counts()[True]\n",
    "accuracy = correct/m\n",
    "print('Accuracy: %.4f' % accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On your own in lab\n",
    "\n",
    "Do the following in lab:\n",
    "1. Write a function to obtain the cost for particular $\\mathbb{X}, \\mathbf{y}, and \\theta$.\n",
    "2. Plot the training set and test cost as training goes on and find the best value for the number of iterations and learning rate.\n",
    "3. Make 2D scatter plots showing the predicted and actual class of each item in the training set, plotting two features at a time.\n",
    "   Comment on the cause of the errors you observe. If you obtain perfect test set accuracy, re-run the train/test split\n",
    "   and rerun the optimization until you observe some mistaken predictions on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On your own to take home\n",
    "\n",
    "We see that the Iris dataset is pretty easy. Depending on the train/test split, we get 97-100% accuracy.\n",
    "\n",
    "Find a more interesting multi-class classification problem on Kaggle, clean the dataset to obtain numerical input features without missing values,\n",
    "split the data into test and train, and experiment with multinomial logistic regression.\n",
    "\n",
    "Write a brief report on your experiments and results. As always, turn in a Jupyter notebook by email\n",
    "to the instructor and TA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
