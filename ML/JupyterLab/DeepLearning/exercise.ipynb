{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "y = np.matrix(data.target).T\n",
    "X = np.matrix(data.data)\n",
    "M = X.shape[0]\n",
    "N = X.shape[1]\n",
    "\n",
    "# Normalize each input feature\n",
    "\n",
    "def normalize(X):\n",
    "    M = X.shape[0]\n",
    "    XX = X - np.tile(np.mean(X,0),[M,1])\n",
    "    XX = np.divide(XX, np.tile(np.std(XX,0),[M,1]))\n",
    "    return XX\n",
    "\n",
    "XX = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Let's start with a 3-layer network with sigmoid activation functions,\n",
    "# 6 units in layer 1, and 5 units in layer 2.\n",
    "h2 = 5\n",
    "h1 = 6\n",
    "\n",
    "W = [[], np.random.normal(0,0.1,[N,h1]),\n",
    "         np.random.normal(0,0.1,[h1,1])]\n",
    "b = [[], np.random.normal(0,0.1,[h1,1]),\n",
    "         np.random.normal(0,0.1,[1,1])]\n",
    "L = len(W)-1\n",
    "print(len(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def actder(z):\n",
    "    az = act(z)\n",
    "    prod = np.multiply(az,1-az)\n",
    "    return prod\n",
    "\n",
    "def actder_relu(z):\n",
    "    der = np.ones((z.shape[0],1))\n",
    "    der[z < 0] = 0\n",
    "    return der\n",
    "\n",
    "\n",
    "def ff(x,W,b):\n",
    "    L = len(W)-1\n",
    "    a = x\n",
    "    for l in range(1,L+1):\n",
    "        z = W[l].T*a+b[l]\n",
    "        a = act(z)\n",
    "    return a\n",
    "\n",
    "def loss(y,yhat):\n",
    "    return -((1-y) * np.log(1-yhat) + y * np.log(yhat))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd = np.array([[1],[-2],[3]])\n",
    "actder_relu(asd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss 3.563132\n",
      "Iteration 1 loss 3.407470\n",
      "Iteration 2 loss 2.972012\n",
      "Iteration 3 loss 3.422946\n",
      "Iteration 4 loss 3.107746\n",
      "Iteration 5 loss 3.200714\n",
      "Iteration 6 loss 3.015608\n",
      "Iteration 7 loss 3.303992\n",
      "Iteration 8 loss 3.125736\n",
      "Iteration 9 loss 2.422943\n",
      "Iteration 10 loss 3.281865\n",
      "Iteration 11 loss 2.839812\n",
      "Iteration 12 loss 2.506987\n",
      "Iteration 13 loss 2.909919\n",
      "Iteration 14 loss 3.143997\n",
      "Iteration 15 loss 2.574789\n",
      "Iteration 16 loss 3.011916\n",
      "Iteration 17 loss 2.660247\n",
      "Iteration 18 loss 2.665575\n",
      "Iteration 19 loss 2.284274\n",
      "Iteration 20 loss 2.971305\n",
      "Iteration 21 loss 2.478878\n",
      "Iteration 22 loss 2.396945\n",
      "Iteration 23 loss 2.408013\n",
      "Iteration 24 loss 2.357275\n",
      "Iteration 25 loss 2.595456\n",
      "Iteration 26 loss 2.322123\n",
      "Iteration 27 loss 2.301611\n",
      "Iteration 28 loss 2.502967\n",
      "Iteration 29 loss 2.333570\n",
      "Iteration 30 loss 2.221720\n",
      "Iteration 31 loss 2.315972\n",
      "Iteration 32 loss 2.225939\n",
      "Iteration 33 loss 2.007063\n",
      "Iteration 34 loss 2.144705\n",
      "Iteration 35 loss 2.249719\n",
      "Iteration 36 loss 2.102543\n",
      "Iteration 37 loss 1.997712\n",
      "Iteration 38 loss 2.147121\n",
      "Iteration 39 loss 2.218944\n",
      "Iteration 40 loss 2.047291\n",
      "Iteration 41 loss 2.136195\n",
      "Iteration 42 loss 1.951550\n",
      "Iteration 43 loss 1.997350\n",
      "Iteration 44 loss 2.000524\n",
      "Iteration 45 loss 1.899756\n",
      "Iteration 46 loss 1.860538\n",
      "Iteration 47 loss 1.960902\n",
      "Iteration 48 loss 1.815456\n",
      "Iteration 49 loss 1.807542\n",
      "Iteration 50 loss 1.872481\n",
      "Iteration 51 loss 1.627162\n",
      "Iteration 52 loss 1.890379\n",
      "Iteration 53 loss 1.803159\n",
      "Iteration 54 loss 1.716918\n",
      "Iteration 55 loss 1.688238\n",
      "Iteration 56 loss 1.675016\n",
      "Iteration 57 loss 1.640640\n",
      "Iteration 58 loss 1.754174\n",
      "Iteration 59 loss 1.598848\n",
      "Iteration 60 loss 1.588014\n",
      "Iteration 61 loss 1.520101\n",
      "Iteration 62 loss 1.731651\n",
      "Iteration 63 loss 1.479242\n",
      "Iteration 64 loss 1.547208\n",
      "Iteration 65 loss 1.586707\n",
      "Iteration 66 loss 1.527078\n",
      "Iteration 67 loss 1.454664\n",
      "Iteration 68 loss 1.538064\n",
      "Iteration 69 loss 1.533905\n",
      "Iteration 70 loss 1.417884\n",
      "Iteration 71 loss 1.440966\n",
      "Iteration 72 loss 1.502287\n",
      "Iteration 73 loss 1.426511\n",
      "Iteration 74 loss 1.416438\n",
      "Iteration 75 loss 1.337810\n",
      "Iteration 76 loss 1.447105\n",
      "Iteration 77 loss 1.369860\n",
      "Iteration 78 loss 1.326700\n",
      "Iteration 79 loss 1.357472\n",
      "Iteration 80 loss 1.348427\n",
      "Iteration 81 loss 1.357886\n",
      "Iteration 82 loss 1.268189\n",
      "Iteration 83 loss 1.325052\n",
      "Iteration 84 loss 1.339460\n",
      "Iteration 85 loss 1.288183\n",
      "Iteration 86 loss 1.233043\n",
      "Iteration 87 loss 1.264703\n",
      "Iteration 88 loss 1.222824\n",
      "Iteration 89 loss 1.258010\n",
      "Iteration 90 loss 1.245563\n",
      "Iteration 91 loss 1.202075\n",
      "Iteration 92 loss 1.212448\n",
      "Iteration 93 loss 1.193978\n",
      "Iteration 94 loss 1.172944\n",
      "Iteration 95 loss 1.178506\n",
      "Iteration 96 loss 1.196195\n",
      "Iteration 97 loss 1.098285\n",
      "Iteration 98 loss 1.174129\n",
      "Iteration 99 loss 1.152793\n"
     ]
    }
   ],
   "source": [
    "# Use mini-batch size 1\n",
    "\n",
    "alpha = 0.1\n",
    "max_iter = 100\n",
    "for iter in range(0, max_iter):\n",
    "    loss_this_iter = 0\n",
    "    order = np.random.permutation(M)\n",
    "    for i in range(0,M):\n",
    "        \n",
    "        # Grab the pattern order[i]\n",
    "        \n",
    "        x_this = XX[order[i],:].T\n",
    "        y_this = y[order[i],0]\n",
    "\n",
    "        # Feed forward step\n",
    "        \n",
    "        a = [x_this]\n",
    "        z = [[]]\n",
    "        delta = [[]]\n",
    "        dW = [[]]\n",
    "        db = [[]]\n",
    "        for l in range(1,L+1):\n",
    "            z.append(W[l].T*a[l-1]+b[l])\n",
    "            a.append(act(z[l]))\n",
    "            # Just to give arrays the right shape for the backprop step\n",
    "            delta.append([]); dW.append([]); db.append([])\n",
    "\n",
    "        loss_this_pattern = loss(y_this, a[L][0,0])\n",
    "        loss_this_iter = loss_this_iter + loss_this_pattern\n",
    "            \n",
    "        # Backprop step\n",
    "\n",
    "        delta[L] = a[L] - y_this\n",
    "        for l in range(L,0,-1):\n",
    "            db[l] = delta[l].copy()\n",
    "            dW[l] = a[l-1] * delta[l].T\n",
    "            if l == L:\n",
    "                delta[l-1] = np.multiply(actder(z[l-1]), W[l] * delta[l])\n",
    "            elif l > 1:\n",
    "                delta[l-1] = np.multiply(actder_relu(z[l-1]), W[l] * delta[l])\n",
    "                \n",
    "        # Check delta calculation\n",
    "        \n",
    "        if False:\n",
    "            print('Target: %f' % y_this)\n",
    "            print('y_hat: %f' % a[L][0,0])\n",
    "            print(db)\n",
    "            y_pred = ff(x_this,W,b)\n",
    "            diff = 1e-3\n",
    "            W[1][10,0] = W[1][10,0] + diff\n",
    "            y_pred_db = ff(x_this,W,b)\n",
    "            L1 = loss(y_this,y_pred)\n",
    "            L2 = loss(y_this,y_pred_db)\n",
    "            db_finite_difference = (L2-L1)/diff\n",
    "            print('Original out %f, perturbed out %f' %\n",
    "                 (y_pred[0,0], y_pred_db[0,0]))\n",
    "            print('Theoretical dW %f, calculated db %f' %\n",
    "                  (dW[1][10,0], db_finite_difference[0,0]))\n",
    "        \n",
    "        for l in range(1,L+1):            \n",
    "            W[l] = W[l] - alpha * dW[l]\n",
    "            b[l] = b[l] - alpha * db[l]\n",
    "        \n",
    "    print('Iteration %d loss %f' % (iter, loss_this_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
