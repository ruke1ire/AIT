{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheat sheet\n",
    "\n",
    "#### Models\n",
    "1. Decision Tree\n",
    "2. Random Forest\n",
    "3. Adaboost\n",
    "4. Gradient Boost\n",
    "5. K-Means\n",
    "6. Gaussian Mixed Model\n",
    "7. Neural Network\n",
    "\n",
    "#### Preprocessing\n",
    "1. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self,max_depth):\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    class Node: \n",
    "        def __init__(self, gini, num_samples, num_samples_per_class, predicted_class):\n",
    "            self.gini = gini\n",
    "            self.num_samples = num_samples\n",
    "            self.num_samples_per_class = num_samples_per_class\n",
    "            self.predicted_class = predicted_class\n",
    "            self.feature_index = 0\n",
    "            self.threshold = 0\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            \n",
    "    def find_split(self, X,y,n_classes):\n",
    "        n_samples, n_features = X.shape\n",
    "        if n_samples <= 1:\n",
    "            return None, None\n",
    "\n",
    "        #so it will not have any warning about \"referenced before assignments\"\n",
    "        feature_ix, threshold = None, None\n",
    "\n",
    "        #count the samples for each class\n",
    "        sample_per_class_parent = [np.sum(y == c) for c in range(n_classes)]\n",
    "\n",
    "        # Gini of parent node.\n",
    "        best_gini = 1.0 - sum((n / n_samples) ** 2 for n in sample_per_class_parent)\n",
    "\n",
    "        # Loop through all features.\n",
    "        for feature in range(n_features):\n",
    "            # Sort data along selected feature.\n",
    "            sample_sorted = sorted(X[:, feature]) #[2, 3, 10, 19]\n",
    "            sort_idx = np.argsort(X[:, feature])\n",
    "            y_sorted = y[sort_idx] #[0, 0, 1, 1]\n",
    "            sample_per_class_left = [0] * n_classes\n",
    "            #[0, 0]\n",
    "            sample_per_class_right = sample_per_class_parent.copy() #[2, 2]\n",
    "            # loop through each threshold, 2.5, 6.5, 14.5\n",
    "            for i in range(1, n_samples):\n",
    "                #the class of that sample\n",
    "                c = y_sorted[i - 1] #[0]\n",
    "                #put the sample to the left\n",
    "                \n",
    "                sample_per_class_left[c] += 1\n",
    "                #[1, 0]\n",
    "                #take the sample out from the right\n",
    "                sample_per_class_right[c] -= 1\n",
    "\n",
    "                gini_left = 1.0 - sum((sample_per_class_left[x] / i) ** 2 for x in range(n_classes))\n",
    "\n",
    "                #since left side has already i samples\n",
    "                gini_right = 1.0 - sum(\n",
    "                (sample_per_class_right[x] / (n_samples - i)) ** 2 for x in range(n_classes)\n",
    "                )\n",
    "                #weighted gini\n",
    "                weighted_gini = (i / n_samples) * gini_left + ((n_samples - i)/n_samples) * gini_right\n",
    "\n",
    "                # in case the value are the same, we do not split\n",
    "                # (both have to end up on the same side of a split).\n",
    "                if sample_sorted[i] == sample_sorted[i - 1]:\n",
    "                    continue\n",
    "                if weighted_gini < best_gini:\n",
    "                    best_gini = weighted_gini\n",
    "                    feature_ix = feature\n",
    "                    threshold = (sample_sorted[i] + sample_sorted[i - 1]) / 2\n",
    "\n",
    "        return feature_ix, threshold\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        self.root = self._fit(X,y,len(set(y)))\n",
    "    \n",
    "    def _fit(self,X,y,n_classes,depth = 0):\n",
    "        assert y.dtype == int\n",
    "        \n",
    "        n_samples,n_features = X.shape\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(n_classes)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "\n",
    "        node = self.Node(\n",
    "            gini = 1-sum((np.sum(y==c)/n_samples)**2 for c in range(n_classes)),\n",
    "            predicted_class = predicted_class,\n",
    "            num_samples = y.size,\n",
    "            num_samples_per_class = num_samples_per_class\n",
    "        )\n",
    "\n",
    "        if (depth >= self.max_depth):\n",
    "            return node\n",
    "        \n",
    "        feature, threshold = self.find_split(X,y,n_classes)\n",
    "        if feature is not None:\n",
    "            indices_left = X[:,feature] < threshold\n",
    "            X_left, y_left = X[indices_left], y[indices_left]\n",
    "            #tilde for negation\n",
    "            X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "            #take note for later decision\n",
    "            node.feature_index = feature\n",
    "            node.threshold = threshold\n",
    "            node.left = self._fit(X_left, y_left, n_classes, depth + 1)\n",
    "            node.right = self._fit(X_right, y_right, n_classes, depth + 1)\n",
    "        return node\n",
    "\n",
    "    def predict(self,sample):\n",
    "        tree = self.root\n",
    "        while tree.left:\n",
    "            if sample[tree.feature_index] < tree.threshold:\n",
    "                tree = tree.left \n",
    "            else:\n",
    "                tree = tree.right \n",
    "        return tree.predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_blobs(n_samples=1000, centers=4,random_state=0, cluster_std=1.0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTree(max_depth = 7)\n",
    "\n",
    "tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t [3 0 0 3 0 1 1 3 1 2 0 0 0 1 2 0 1 3 1 3 2 2 1 1 0 1 2 3 3 3 1 3 0 0 3 2 1\n",
      " 1 2 3 3 2 3 2 3 3 0 3 3 2 2 0 3 1 1 1 1 3 0 3 3 3 0 0 1 2 3 2 3 1 3 2 1 0\n",
      " 0 3 2 2 0 0 3 1 3 2 0 2 3 0 1 1 2 3 0 1 3 2 1 2 2 2 1 1 2 2 2 1 3 0 0 0 0\n",
      " 2 3 0 2 2 0 2 2 3 0 0 3 1 1 2 0 1 0 3 2 2 2 3 1 1 1 0 2 3 3 3 1 3 3 1 2 3\n",
      " 2 3 0 1 2 0 1 2 0 1 1 1 0 2 0 3 1 2 0 2 2 0 0 0 2 0 2 3 2 3 3 0 2 2 0 0 1\n",
      " 0 3 2 0 2 2 2 3 2 3 2 3 3 0 2]\n",
      "Y:\t\t [3 0 0 3 0 1 1 3 1 2 0 0 3 1 2 0 1 0 1 3 2 2 1 1 3 1 2 3 3 3 1 3 0 0 3 2 1\n",
      " 1 2 3 3 2 3 2 3 3 0 3 3 2 2 2 3 1 1 1 1 3 0 3 3 3 1 0 1 2 3 2 3 1 3 2 0 0\n",
      " 0 3 2 2 0 0 3 0 3 2 0 2 3 0 1 1 2 3 0 1 3 2 1 2 2 0 1 1 2 2 2 1 3 0 0 0 0\n",
      " 2 3 0 0 2 0 1 2 3 0 0 3 1 1 2 0 0 0 3 2 2 2 3 1 1 1 0 2 3 3 3 1 3 3 1 2 3\n",
      " 2 3 2 1 2 0 1 0 0 1 1 1 0 2 0 3 1 2 0 2 2 0 0 0 0 1 2 3 2 0 3 0 2 2 0 2 1\n",
      " 0 3 2 2 2 2 2 3 2 3 0 3 3 0 2]\n",
      "Accuracy 90.5\n"
     ]
    }
   ],
   "source": [
    "pred = [tree.predict(x) for x in X_test]\n",
    "print(\"Prediction:\\t\",np.array(pred))\n",
    "print(\"Y:\\t\\t\", y_test)\n",
    "print(\"Accuracy\", 100*np.sum(y_test==np.array(pred))/y_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t [3 0 0 3 0 1 1 3 1 2 0 0 0 1 2 0 0 3 1 3 2 2 1 1 0 1 2 3 3 3 1 3 0 0 3 2 1\n",
      " 1 2 3 3 2 3 2 3 3 0 3 3 2 2 0 3 1 1 1 1 3 0 3 3 3 0 0 1 2 3 2 3 0 3 2 0 0\n",
      " 0 3 2 2 0 0 3 0 3 2 0 2 3 0 1 1 2 3 0 1 3 2 1 2 2 2 1 1 2 2 2 1 3 0 0 0 0\n",
      " 2 3 0 2 2 0 2 2 3 0 0 3 1 1 2 0 1 0 3 2 2 2 3 1 1 1 0 2 3 3 3 1 3 3 1 2 3\n",
      " 2 3 0 1 2 0 1 2 0 1 1 1 0 2 0 3 1 2 0 2 2 0 0 0 2 0 2 3 2 3 3 0 2 2 0 0 1\n",
      " 0 3 2 0 2 2 2 3 2 3 2 3 3 0 2]\n",
      "Y:\t\t [3 0 0 3 0 1 1 3 1 2 0 0 3 1 2 0 1 0 1 3 2 2 1 1 3 1 2 3 3 3 1 3 0 0 3 2 1\n",
      " 1 2 3 3 2 3 2 3 3 0 3 3 2 2 2 3 1 1 1 1 3 0 3 3 3 1 0 1 2 3 2 3 1 3 2 0 0\n",
      " 0 3 2 2 0 0 3 0 3 2 0 2 3 0 1 1 2 3 0 1 3 2 1 2 2 0 1 1 2 2 2 1 3 0 0 0 0\n",
      " 2 3 0 0 2 0 1 2 3 0 0 3 1 1 2 0 0 0 3 2 2 2 3 1 1 1 0 2 3 3 3 1 3 3 1 2 3\n",
      " 2 3 2 1 2 0 1 0 0 1 1 1 0 2 0 3 1 2 0 2 2 0 0 0 0 1 2 3 2 0 3 0 2 2 0 2 1\n",
      " 0 3 2 2 2 2 2 3 2 3 0 3 3 0 2]\n",
      "Accuracy 90.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=7).fit(X_train,y_train)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "print(\"Prediction:\\t\",np.array(pred))\n",
    "print(\"Y:\\t\\t\", y_test)\n",
    "print(\"Accuracy\", 100*np.sum(y_test==np.array(pred))/y_test.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, no_of_trees = 1, max_depth = 3):\n",
    "        self.no_of_trees = no_of_trees\n",
    "        self.trees = [DecisionTree(max_depth = max_depth) for _ in range(no_of_trees)]\n",
    "        \n",
    "    def bagging(self,X,y):\n",
    "        m,n = X.shape\n",
    "        \n",
    "        x_samples = np.zeros((self.no_of_trees, m, n))\n",
    "        y_samples = np.zeros((self.no_of_trees,m),dtype=y.dtype)\n",
    "\n",
    "        for i in range(self.no_of_trees):\n",
    "            for j in range(m):\n",
    "                idx = np.random.randint(m)\n",
    "                x_samples[i,j,:] = X_train[idx]\n",
    "                y_samples[i,j] = y_train[idx]\n",
    "        \n",
    "        return x_samples, y_samples\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        x_samples, y_samples = self.bagging(X,y)\n",
    "        \n",
    "        for i, tree in enumerate(self.trees):\n",
    "            _X = x_samples[i, :]\n",
    "            _y = y_samples[i,:]\n",
    "            tree.fit(_X,_y)\n",
    "    \n",
    "    def predict(self,X):\n",
    "        from scipy import stats\n",
    "        predictions = np.zeros((self.no_of_trees,X.shape[0]))\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            yhat = np.array([tree.predict(x) for x in X])\n",
    "            predictions[i,:] = yhat\n",
    "        pred = stats.mode(predictions)[0][0].astype(int)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t [0 0 0 3 0 1 1 3 1 2 0 0 0 1 2 0 0 3 1 3 2 2 1 1 0 1 2 3 3 3 1 3 0 0 3 2 1\n",
      " 1 2 3 3 2 3 2 3 3 0 3 3 2 2 0 3 1 1 1 1 3 0 3 3 3 1 0 1 2 3 2 3 1 3 2 0 0\n",
      " 0 3 2 2 0 0 3 0 3 2 0 2 3 0 1 1 2 3 0 1 3 2 1 2 2 2 1 1 2 2 2 1 3 0 0 0 0\n",
      " 2 3 0 2 2 0 2 2 3 0 0 3 1 1 2 0 1 0 3 2 2 2 3 1 1 1 0 2 3 3 3 1 3 3 1 2 3\n",
      " 2 3 2 1 2 0 1 2 0 1 1 1 0 2 0 3 1 2 0 2 2 0 0 0 0 0 2 3 2 3 3 0 2 2 0 2 1\n",
      " 0 3 2 0 2 2 2 3 2 3 2 3 3 0 2]\n",
      "Y:\t\t [3 0 0 3 0 1 1 3 1 2 0 0 3 1 2 0 1 0 1 3 2 2 1 1 3 1 2 3 3 3 1 3 0 0 3 2 1\n",
      " 1 2 3 3 2 3 2 3 3 0 3 3 2 2 2 3 1 1 1 1 3 0 3 3 3 1 0 1 2 3 2 3 1 3 2 0 0\n",
      " 0 3 2 2 0 0 3 0 3 2 0 2 3 0 1 1 2 3 0 1 3 2 1 2 2 0 1 1 2 2 2 1 3 0 0 0 0\n",
      " 2 3 0 0 2 0 1 2 3 0 0 3 1 1 2 0 0 0 3 2 2 2 3 1 1 1 0 2 3 3 3 1 3 3 1 2 3\n",
      " 2 3 2 1 2 0 1 0 0 1 1 1 0 2 0 3 1 2 0 2 2 0 0 0 0 1 2 3 2 0 3 0 2 2 0 2 1\n",
      " 0 3 2 2 2 2 2 3 2 3 0 3 3 0 2]\n",
      "Accuracy 92.5\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForest(no_of_trees = 20, max_depth = 5)\n",
    "\n",
    "forest.fit(X_train,y_train)\n",
    "\n",
    "pred = forest.predict(X_test)\n",
    "\n",
    "print(\"Prediction:\\t\",np.array(pred))\n",
    "print(\"Y:\\t\\t\", y_test)\n",
    "print(\"Accuracy\", 100*np.sum(y_test==np.array(pred))/y_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t [0 0 0 3 0 1 1 3 1 2 0 0 0 1 2 0 1 3 1 3 2 2 1 1 0 1 2 3 3 3 1 3 0 0 3 2 1\n",
      " 1 2 3 3 2 3 0 3 3 0 3 3 2 2 0 3 1 1 1 1 3 0 3 3 3 1 0 1 2 3 2 3 1 3 2 1 0\n",
      " 0 3 2 2 0 0 3 0 3 2 0 2 3 0 1 1 2 3 0 1 3 2 1 2 2 2 1 1 2 2 2 1 3 0 0 0 0\n",
      " 2 3 0 2 2 0 2 2 3 0 0 3 1 1 2 0 1 0 3 2 2 2 3 1 1 1 0 2 3 3 3 1 3 3 1 2 3\n",
      " 2 3 2 1 2 0 1 2 0 1 1 1 0 2 0 3 1 2 0 2 2 0 1 0 0 0 2 3 2 3 3 0 2 2 0 2 1\n",
      " 0 3 2 0 2 2 2 3 2 3 2 3 3 0 2]\n",
      "Y:\t\t [3 0 0 3 0 1 1 3 1 2 0 0 3 1 2 0 1 0 1 3 2 2 1 1 3 1 2 3 3 3 1 3 0 0 3 2 1\n",
      " 1 2 3 3 2 3 2 3 3 0 3 3 2 2 2 3 1 1 1 1 3 0 3 3 3 1 0 1 2 3 2 3 1 3 2 0 0\n",
      " 0 3 2 2 0 0 3 0 3 2 0 2 3 0 1 1 2 3 0 1 3 2 1 2 2 0 1 1 2 2 2 1 3 0 0 0 0\n",
      " 2 3 0 0 2 0 1 2 3 0 0 3 1 1 2 0 0 0 3 2 2 2 3 1 1 1 0 2 3 3 3 1 3 3 1 2 3\n",
      " 2 3 2 1 2 0 1 0 0 1 1 1 0 2 0 3 1 2 0 2 2 0 0 0 0 1 2 3 2 0 3 0 2 2 0 2 1\n",
      " 0 3 2 2 2 2 2 3 2 3 0 3 3 0 2]\n",
      "Accuracy 91.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(\"Prediction:\\t\",np.array(pred))\n",
    "print(\"Y:\\t\\t\", y_test)\n",
    "print(\"Accuracy\", 100*np.sum(y_test==np.array(pred))/y_test.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=500, random_state=1)\n",
    "\n",
    "y = np.where(y==0,-1,1) #change our y to be -1 if it is 0, otherwise 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      0.97      0.97        79\n",
      "           1       0.97      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.97       150\n",
      "   macro avg       0.97      0.97      0.97       150\n",
      "weighted avg       0.97      0.97      0.97       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "m = X_train.shape[0]\n",
    "S = 20\n",
    "\n",
    "stump_params = {'max_depth': 1, 'max_leaf_nodes': 2}\n",
    "models = [DecisionTreeClassifier(**stump_params) for _ in range(S)]\n",
    "\n",
    "# weight for each sample\n",
    "W = np.full(m, 1/m)\n",
    "\n",
    "# weight for each classifier\n",
    "a_js = np.zeros(S)\n",
    "\n",
    "for j, model in enumerate(models):\n",
    "    #train weak learner\n",
    "    model.fit(X_train, y_train, sample_weight = W)\n",
    "    #compute the errors\n",
    "    yhat = model.predict(X_train)\n",
    "    err = W[(yhat != y_train)].sum()\n",
    "    #compute the predictor weight a_j\n",
    "    #if predictor is doing well, a_j will be big\n",
    "    a_j = np.log ((1 - err) / err) / 2\n",
    "    a_js[j] = a_j\n",
    "    #update sample weight; divide sum of W to normalize\n",
    "    W = (W * np.exp(-a_j * y_train * yhat))\n",
    "    W = W/sum(W)\n",
    "    \n",
    "Hx = 0\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    yhat = model.predict(X_test)\n",
    "    Hx += a_js[i] * yhat\n",
    "    \n",
    "yhat = np.sign(Hx)\n",
    "\n",
    "print(classification_report(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      0.97      0.97        79\n",
      "           1       0.97      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.97       150\n",
      "   macro avg       0.97      0.97      0.97       150\n",
      "weighted avg       0.97      0.97      0.97       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200,learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "def grad(y, h):\n",
    "    return y - h\n",
    "\n",
    "def fit(X, y, models):\n",
    "    models_trained = []\n",
    "    \n",
    "    #using DummyRegressor is a good technique for starting model\n",
    "    first_model = DummyRegressor(strategy='mean')\n",
    "    first_model.fit(X, y)\n",
    "    models_trained.append(first_model)\n",
    "    \n",
    "    #fit the estimators\n",
    "    for i, model in enumerate(models):\n",
    "        #predict using all the weak learners we trained up to\n",
    "        #this point\n",
    "        y_pred = predict(X, models_trained)\n",
    "        \n",
    "        #errors will be the total errors maded by models_trained\n",
    "        residual = grad(y, y_pred)\n",
    "        \n",
    "        #fit the next model with residual\n",
    "        model.fit(X, residual)\n",
    "        models_trained.append(model)\n",
    "        \n",
    "    return models_trained\n",
    "\n",
    "def predict(X, models):\n",
    "    learning_rate = 0.1 ##hard code for now\n",
    "    f0 = models[0].predict(X) #first use the dummy model\n",
    "    boosting = sum(learning_rate * model.predict(X) for model in models[1:])\n",
    "    return f0 + boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortion = []\n",
    "nearests = []\n",
    "k_list = []\n",
    "mean_list = []\n",
    "\n",
    "for k in range(2,10):\n",
    "    k_list.append(k)\n",
    "    distortion.append(float('inf'))\n",
    "    nearests.append(0)\n",
    "    mean_list.append(0)\n",
    "    for tries in range(10):\n",
    "        centers = np.random.uniform(2,4,size=(k,X.shape[1]))\n",
    "        while True:\n",
    "            #find the nearest centers for each of the points\n",
    "            distance = np.empty((X.shape[0],centers.shape[0]))\n",
    "            for i,x in enumerate(X):\n",
    "                for j,c in enumerate(centers):\n",
    "                    distance[i,j] = (c-x).T@(c-x) \n",
    "            nearest = np.argmin(distance,axis=1)\n",
    "\n",
    "            #find the mean of each centers\n",
    "            mean = centers.copy()\n",
    "            for i in np.unique(nearest):\n",
    "                mean[i] = np.mean(X[nearest==i],axis=0)\n",
    "\n",
    "            if(np.sum((mean-centers)**2)<1e-6):\n",
    "                dis = np.sum(np.min(distance,axis=1))\n",
    "                if(distortion[-1]>dis):\n",
    "                    distortion[-1] = dis\n",
    "                    nearests[-1] = nearest\n",
    "                    mean_list[-1] = mean\n",
    "                break\n",
    "            else:\n",
    "                centers = mean\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(k_list,distortion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gaussian Mixed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
