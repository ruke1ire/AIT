{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "    # nothing to initialize\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    #forward function receives an ndarray as input\n",
    "    def forward(self, input_:ndarray) -> ndarray:\n",
    "        self.input_ = input_\n",
    "        \n",
    "        self.output = self._output()\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad):\n",
    "        assert self.output.shape == output_grad.shape\n",
    "        \n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        \n",
    "        assert self.input_.shape == self.input_grad.shape\n",
    "        \n",
    "        return self.input_grad\n",
    "    \n",
    "    def _output(self):\n",
    "        raise NotImplementError()\n",
    "        \n",
    "    def _input_grad(self, output_grad):\n",
    "        raise NotImplementError()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamOperation(Operation):\n",
    "    def __init__(self, param: ndarray):\n",
    "        super().__init__() #inherit from parent if any\n",
    "        self.param = param #this will be used in _output\n",
    "        \n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        #make sure output and output_grad has same shape\n",
    "        assert self.output.shape == output_grad.shape\n",
    "        \n",
    "        #perform gradients for both input and param\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "        \n",
    "        assert self.input_.shape == self.input_grad.shape\n",
    "        assert self.param.shape == self.param_grad.shape\n",
    "        \n",
    "        return self.input_grad\n",
    "    \n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOperation):\n",
    "    def __init__(self, W: ndarray):\n",
    "        super().__init__(W)\n",
    "    \n",
    "    def _output(self):\n",
    "        return self.input_ @ self.param\n",
    "    \n",
    "    def _input_grad(self,output_grad):\n",
    "        #this is because dOut/dX = W.T and since the equation is X@W, W.T is post multiplied\n",
    "        return output_grad @ self.param.T\n",
    "    \n",
    "    def _param_grad(self, output_grad):\n",
    "        #since dOut/dW = X.T and since the equation is X@W, X.T is pre multiplied\n",
    "        return self.input_.T @ output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdd(ParamOperation):\n",
    "    def __init__(self, B):\n",
    "        assert B.shape[0] == 1\n",
    "        super().__init__(B)\n",
    "        \n",
    "    def _output(self):\n",
    "        return self.input_ + self.param\n",
    "    \n",
    "    def _input_grad(self, output_grad):\n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "    \n",
    "    def _param_grad(self, output_grad):\n",
    "        param_grad = np.ones_like(self.param)*output_grad\n",
    "        return np.sum(param_grad,axis=0).reshape(1,param_grad.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def _output(self):\n",
    "        return 1.0/(1.0+np.exp(-1.0*self.input_))\n",
    "    \n",
    "    def _input_grad(self, output_grad):\n",
    "        sigmoid_backward = self.output * (1.0 - self.output)\n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        return input_grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Operation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self):\n",
    "        return self.input_\n",
    "    \n",
    "    def _input_grad(self, output_grad):\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, neurons):\n",
    "        self.neurons = neurons\n",
    "        self.first = True\n",
    "        self.parmas = []\n",
    "        self.param_grads = []\n",
    "        self.operations = []\n",
    "        \n",
    "    #setup layer is to initialize stuff such as the parameter values, etc\n",
    "    def _setup_layer(self,num_in):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "        \n",
    "        self.input_ = input_\n",
    "        \n",
    "        for operation in self.operations:\n",
    "            input_ = operation.forward(input_)\n",
    "            \n",
    "        self.output = input_\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad):\n",
    "        assert self.output.shape == output_grad.shape\n",
    "        \n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "            \n",
    "        input_grad = output_grad\n",
    "        \n",
    "        self._param_grads()\n",
    "        \n",
    "        return input_grad\n",
    "    \n",
    "    def _param_grads(self):\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "    \n",
    "    def _params(self):\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self,neurons: int, activation: Operation = Sigmoid()):\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "    \n",
    "    def _setup_layer(self,input_: ndarray):\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "        \n",
    "        self.params = []\n",
    "    \n",
    "        self.params.append(np.random.randn(input_.shape[1], self.neurons))\n",
    "        \n",
    "        self.params.append(np.random.randn(1, self.neurons))\n",
    "        \n",
    "        self.operations = [WeightMultiply(self.params[0]),BiasAdd(self.params[1]),self.activation]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, prediction: ndarray, target: ndarray) -> float:\n",
    "        assert prediction.shape == target.shape\n",
    "        \n",
    "        self.prediction = prediction \n",
    "        self.target = target\n",
    "        \n",
    "        loss_value = self._output()\n",
    "        \n",
    "        return loss_value\n",
    "    \n",
    "    def backward(self) -> ndarray:\n",
    "        \n",
    "        self.input_grad = self._input_grad()\n",
    "        \n",
    "        assert self.prediction.shape == self.input_grad.shape\n",
    "        \n",
    "        return self.input_grad\n",
    "    \n",
    "    def _output(self) -> float:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _input_grad(self) -> ndarray:\n",
    "        raise NotImplementedError()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def _output(self) -> float:\n",
    "        loss = (np.sum(np.power(self.prediction - self.target, 2)) / self.prediction.shape[0])\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def _input_grad(self) -> ndarray:\n",
    "        return 2.0 * (self.prediction - self.target) / self.prediction.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers: List[Layer], loss: Loss, seed: int = 1):\n",
    "        self.layers = layers \n",
    "        self.loss = loss\n",
    "        self.seed = seed\n",
    "        if seed:\n",
    "            for layer in self.layers:\n",
    "                setattr(layer, \"seed\", self.seed)\n",
    "                \n",
    "    def forward(self, x_batch):\n",
    "        x_out = x_batch\n",
    "        for layer in self.layers:\n",
    "            x_out = layer.forward(x_out)\n",
    "            \n",
    "        return x_out\n",
    "    \n",
    "    def backward(self, loss_grad: ndarray):\n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "    def train_batch(self, x_batch: ndarray, y_batch:ndarray) -> float:\n",
    "        predictions = self.forward(x_batch)\n",
    "        loss = self.loss.forward(predictions, y_batch)\n",
    "        self.backward(self.loss.backward())\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def params(self):\n",
    "        for layer in self.layers:\n",
    "            yield from layer.params\n",
    "            \n",
    "    def param_grads(self):\n",
    "        for layer in self.layers:\n",
    "            yield from layer.param_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = NeuralNetwork(layers=[Dense(neurons=1,activation=Linear())], \n",
    "                   loss=MeanSquaredError(), \n",
    "                   seed = 20200720)\n",
    "\n",
    "nn = NeuralNetwork(layers=[Dense(neurons=13, activation=Sigmoid()),\n",
    "                           Dense(neurons=1,activation=Linear())], \n",
    "                   loss=MeanSquaredError(), \n",
    "                   seed = 20200720)\n",
    "\n",
    "d1 = NeuralNetwork(layers=[Dense(neurons=13, activation=Sigmoid()),\n",
    "                           Dense(neurons=13,activation=Sigmoid()),\n",
    "                           Dense(neurons=1,activation=Linear())],\n",
    "                   loss=MeanSquaredError(), \n",
    "                   seed=20200720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, lr: float = 0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr: float = 0.01):\n",
    "        super().__init__(lr)\n",
    "        \n",
    "    def step(self):\n",
    "        for (param, param_grad) in zip(self.net.params(), self.net.param_grads()):\n",
    "            param -= self.lr * param_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self,net:NeuralNetowrk, optim:Optimizer):\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.best_loss = 1e9\n",
    "        \n",
    "        setattr(self.optim, 'net', self.net)\n",
    "    \n",
    "    def permute_data(self,X,y):\n",
    "        perm = np.random.permutation(X.shape[0])\n",
    "        return X[perm], y[perm]\n",
    "    \n",
    "    def generate_batches(self, X:ndarray, y:ndarray, size: int = 32) -> Tuple[ndarray]:\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "        \n",
    "        N = X.shape[0]\n",
    "        \n",
    "        for i in range(0, N, size):\n",
    "            X_batch, y_batch = X[i:i+size], y[i:i+size]\n",
    "            yield X_batch, y_batch\n",
    "            \n",
    "    def fit(self, X_train: ndarray, y_train: ndarray,\n",
    "           X_test: ndarray, y_test: ndarray,\n",
    "           epochs: int = 100,\n",
    "           eval_every: int = 10,\n",
    "           batch_size: int = 32,\n",
    "           seed: int = 1, \n",
    "           restart: bool = True):\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "                \n",
    "            self.best_loss = 1e9\n",
    "            \n",
    "        for e in range(epochs):\n",
    "            if (e+1) % eval_every == 0:\n",
    "                last_model = deepcopy(self.net)\n",
    "                \n",
    "            X_train, y_train = self.permute_data(X_train, y_train)\n",
    "            \n",
    "            batch_generator = self.generate_batches(X_train, y_train, batch_size)\n",
    "            \n",
    "            for (X_batch, y_batch) in batch_generator:\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "                \n",
    "                self.optim.step()\n",
    "                \n",
    "            if (e+1) % eval_every== 0:\n",
    "                test_preds = self.net.forward(X_test)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "                \n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "                    self.best_loss = loss\n",
    "                else:\n",
    "                    print(f\"Loss increased after epoch {e+1}\")\n",
    "                    self.net = last_model \n",
    "                    setattr(self.optim, 'net', self.net)\n",
    "                    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "features = boston.feature_names\n",
    "s = StandardScaler()\n",
    "X = s.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42)\n",
    "\n",
    "y_train, y_test = y_train.reshape(-1,1), y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 26.549\n",
      "Validation loss after 20 epochs is 24.722\n",
      "Validation loss after 30 epochs is 22.859\n",
      "Validation loss after 40 epochs is 22.766\n",
      "Validation loss after 50 epochs is 22.648\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(lr, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test, epochs = 50, eval_every = 10, seed = 20200720)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.647618498359957"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = lr.forward(X_test)\n",
    "mean_squared_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 23.608\n",
      "Validation loss after 20 epochs is 21.098\n",
      "Validation loss after 30 epochs is 15.430\n",
      "Validation loss after 40 epochs is 13.955\n",
      "Validation loss after 50 epochs is 13.393\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(nn, SGD(lr = 0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test, epochs = 50, eval_every = 10, seed = 20200720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN MSE:  13.392638894503875\n"
     ]
    }
   ],
   "source": [
    "preds = nn.forward(X_test)\n",
    "print(\"NN MSE: \", mean_squared_error(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 31.474\n",
      "Validation loss after 20 epochs is 19.227\n",
      "Validation loss after 30 epochs is 15.901\n",
      "Validation loss after 40 epochs is 14.381\n",
      "Validation loss after 50 epochs is 12.854\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(d1, SGD(lr = 0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test, epochs = 50, eval_every = 10, seed = 20200720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL MSE:  12.853543518253922\n"
     ]
    }
   ],
   "source": [
    "preds = d1.forward(X_test)\n",
    "print(\"DL MSE: \", mean_squared_error(y_test, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
