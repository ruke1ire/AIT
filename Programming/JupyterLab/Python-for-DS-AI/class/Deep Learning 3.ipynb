{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralnet.first_version import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "\n",
    "class SoftmaxCrossEntropy(Loss):\n",
    "    def __init__(self, eps: float=1e-9):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    \n",
    "    def _output(self) -> float:\n",
    "        softmax_preds = self.softmax(self.prediction, axis=1)\n",
    "        \n",
    "        self.softmax_preds = np.clip(softmax_preds, self.eps, 1-self.eps)\n",
    "        \n",
    "        softmax_cross_entropy_loss = (-1.0* self.target * np.log(self.softmax_preds))\n",
    "        \n",
    "        return np.sum(softmax_cross_entropy_loss) / self.prediction.shape[0]\n",
    "    \n",
    "    def _input_grad(self) -> ndarray:\n",
    "        return (self.softmax_preds - self.target) / self.prediction.shape[0]\n",
    "    \n",
    "    def softmax(self, x, axis=None):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=axis, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x,np.zeros_like(x))\n",
    "\n",
    "def relu_derivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$ReLU(x)$')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-5, 5, 0.01)\n",
    "plt.plot(x, relu(x))\n",
    "# plt.plot(x, relu_derivative(x))\n",
    "plt.title(\"ReLU activation function\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"$ReLU(x)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Operation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self) -> ndarray:\n",
    "        return np.tanh(self.input_)\n",
    "    \n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        return output_grad * (1-self.output*self.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version = 1, cache = True)\n",
    "mnist.target = mnist.target.astype(int)\n",
    "\n",
    "X_train = mnist['data'][:60000]\n",
    "y_train = mnist['target'][:60000]\n",
    "\n",
    "X_test = mnist['data'][60000:]\n",
    "y_test = mnist['target'][60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "onehot = preprocessing.OneHotEncoder()\n",
    "\n",
    "#sklearn expects a 2D array thus we have to reshape to (-1, 1)\n",
    "y_train_encode = onehot.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test_encode = onehot.fit_transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "print(y_train_encode.shape, y_test_encode.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calc_accuracy(model, X_test, y_test):    \n",
    "    #getting the accuracy score with testing data\n",
    "    preds = model.forward(X_test)\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    print(\"Accuracy: \", accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers = [Dense(neurons=89,activation=Tanh()),\n",
    "             Dense(neurons=10,activation=Sigmoid())],\n",
    "    loss = MeanSquaredError(),\n",
    "    seed = 20200720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.300\n",
      "Validation loss after 20 epochs is 0.245\n",
      "Validation loss after 30 epochs is 0.225\n",
      "Validation loss after 40 epochs is 0.216\n",
      "Validation loss after 50 epochs is 0.210\n",
      "Accuracy:  0.8758\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, SGD(0.1))\n",
    "trainer.fit(X_train, y_train_encode, X_test, y_test_encode, epochs = 50, eval_every = 10, seed=20200720, batch_size=60)\n",
    "\n",
    "calc_accuracy(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89,activation=Tanh()),\n",
    "           Dense(neurons=10,activation=Linear())],\n",
    "    loss = SoftmaxCrossEntropy(),\n",
    "    seed=20200720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.413\n",
      "Validation loss after 20 epochs is 0.390\n",
      "Validation loss after 30 epochs is 0.384\n",
      "Loss increased after epoch 40, final loss was 0.384, using the model from epoch 30\n",
      "Accuracy:  0.8898\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model,SGD(0.1))\n",
    "trainer.fit(X_train, y_train_encode, X_test, y_test_encode,\n",
    "           epochs= 50,\n",
    "           eval_every = 10,\n",
    "           seed = 20200720,\n",
    "           batch_size = 60)\n",
    "\n",
    "calc_accuracy(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(self, lr: float = 0.01, momentum: float = 0.9):\n",
    "        super().__init__(lr)\n",
    "        self.momentum = momentum\n",
    "        self.first = True\n",
    "    \n",
    "    def step(self):\n",
    "        if self.first:\n",
    "            self.velocities = [np.zeros_like(param) for param in self.net.params()]\n",
    "            self.first = False\n",
    "            \n",
    "        for (param, param_grad, velocity) in zip(self.net.params(), self.net.param_grads(), self.velocities):\n",
    "            self._update_rule(param=param, grad=param_grad,velocity=velocity)\n",
    "        \n",
    "    def _update_rule(self,**kwargs):\n",
    "        kwargs['velocity'] *= self.momentum\n",
    "        kwargs['velocity'] += self.lr * kwargs['grad']\n",
    "        kwargs['param'] -= kwargs['velocity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.349\n",
      "Loss increased after epoch 20, final loss was 0.349, using the model from epoch 10\n",
      "Accuracy:  0.9156\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(layers = [Dense(neurons=89, activation=Tanh()),\n",
    "                               Dense(neurons=10, activation=Linear())],\n",
    "                      loss = SoftmaxCrossEntropy(),\n",
    "                      seed = 20200720)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(lr=0.1, momentum=0.9))\n",
    "trainer.fit(X_train, y_train_encode, X_test, y_test_encode, \n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed = 20200720,\n",
    "            batch_size = 60)\n",
    "\n",
    "calc_accuracy(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01,\n",
    "                 final_lr: float = 0,\n",
    "                 decay_type: str = 'exponential'):\n",
    "        self.lr = lr\n",
    "        self.final_lr = final_lr  #<----added\n",
    "        self.decay_type = decay_type #<----added\n",
    "\n",
    "    def _setup_decay(self):  #<----added\n",
    "\n",
    "        if not self.decay_type:\n",
    "            return\n",
    "        elif self.decay_type == 'exponential':\n",
    "            self.decay_per_epoch = np.power(self.final_lr / self.lr,\n",
    "                                       1.0 / (self.max_epochs - 1))\n",
    "        elif self.decay_type == 'linear':\n",
    "            self.decay_per_epoch = (self.lr - self.final_lr) / (self.max_epochs - 1)\n",
    "\n",
    "    def _decay_lr(self): #<----added\n",
    "\n",
    "        if not self.decay_type:\n",
    "            return\n",
    "\n",
    "        if self.decay_type == 'exponential':\n",
    "            self.lr *= self.decay_per_epoch\n",
    "\n",
    "        elif self.decay_type == 'linear':\n",
    "            self.lr -= self.decay_per_epoch\n",
    "\n",
    "    def step(self, epoch: int = 0):  #<----added epoch info\n",
    "\n",
    "        for (param, param_grad) in zip(self.net.params(),\n",
    "                                       self.net.param_grads()):\n",
    "            self._update_rule(param=param,\n",
    "                              grad=param_grad)\n",
    "\n",
    "    def _update_rule(self, **kwargs):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01,\n",
    "                 final_lr: float = 0,   #<----added\n",
    "                 decay_type: str = None,   #<------added\n",
    "                 momentum: float = 0.9):\n",
    "        super().__init__(lr, final_lr, decay_type)   #<---changed\n",
    "        self.momentum = momentum\n",
    "        self.first = True\n",
    "\n",
    "    def step(self):\n",
    "        if self.first:\n",
    "            self.velocities = [np.zeros_like(param)\n",
    "                               for param in self.net.params()]\n",
    "            self.first = False\n",
    "\n",
    "        for (param, param_grad, velocity) in zip(self.net.params(),\n",
    "                                                 self.net.param_grads(),\n",
    "                                                 self.velocities):\n",
    "            self._update_rule(param=param,\n",
    "                              grad=param_grad,\n",
    "                              velocity=velocity)\n",
    "\n",
    "    def _update_rule(self, **kwargs):\n",
    "\n",
    "        # Update velocity\n",
    "        kwargs['velocity'] *= self.momentum\n",
    "        kwargs['velocity'] += self.lr * kwargs['grad']\n",
    "\n",
    "        # Use this to update parameters\n",
    "        kwargs['param'] -= kwargs['velocity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "\n",
    "class Trainer(object):\n",
    "    #NeuralNetwork and Optimizer as attributes\n",
    "    def __init__(self,\n",
    "                 net: NeuralNetwork,\n",
    "                 optim: Optimizer):\n",
    "        #Requires a neural network and an optimizer in order for \n",
    "        #training to occur. \n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.best_loss = 1e9  #use for comparing the least amount of loss\n",
    "        \n",
    "        #Assign the neural network as an instance variable to \n",
    "        #the optimizer when the code runs\n",
    "        setattr(self.optim, 'net', self.net)\n",
    "    \n",
    "\n",
    "    # helper function for shuffling\n",
    "    def permute_data(self, X, y):\n",
    "        perm = np.random.permutation(X.shape[0])\n",
    "        return X[perm], y[perm]\n",
    "\n",
    "    # helper function for generating batches\n",
    "    def generate_batches(self,\n",
    "                         X: ndarray,\n",
    "                         y: ndarray,\n",
    "                         size: int = 32) -> Tuple[ndarray]:\n",
    "        #X and y should have same number of rows\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for i in range(0, N, size):\n",
    "            X_batch, y_batch = X[i:i+size], y[i:i+size]\n",
    "            #return a generator that can be loop\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "            \n",
    "    def fit(self, X_train: ndarray, y_train: ndarray,\n",
    "            X_test: ndarray, y_test: ndarray,\n",
    "            epochs: int=100,\n",
    "            eval_every: int=10,\n",
    "            batch_size: int=32,\n",
    "            seed: int = 20200720,\n",
    "            restart: bool = True):\n",
    "        \n",
    "        \n",
    "        setattr(self.optim, 'max_epochs', epochs)  #<----added\n",
    "        self.optim._setup_decay() #<----added\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        #for resetting\n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "\n",
    "            self.best_loss = 1e9\n",
    "        \n",
    "        #Fits the neural network on the training data for a certain \n",
    "        #number of epochs.\n",
    "        for e in range(epochs):\n",
    "            \n",
    "            if (e+1) % eval_every == 0:\n",
    "                \n",
    "                # for early stopping\n",
    "                # deepcopy is a hardcopy function that make sure it construct a new object (copy() is a shallow copy)\n",
    "                last_model = deepcopy(self.net)\n",
    "\n",
    "            X_train, y_train = self.permute_data(X_train, y_train)\n",
    "\n",
    "            batch_generator = self.generate_batches(X_train, y_train,\n",
    "                                                    batch_size)\n",
    "\n",
    "            for (X_batch, y_batch) in batch_generator:\n",
    "\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "\n",
    "                self.optim.step()\n",
    "            \n",
    "            #Every \"eval_every\" epochs, it evaluated the neural network \n",
    "            #on the testing data.\n",
    "            if (e+1) % eval_every == 0:\n",
    "\n",
    "                test_preds = self.net.forward(X_test)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "\n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "                    self.best_loss = loss\n",
    "                #if the validation loss is not lower, it stop and perform early stopping\n",
    "                else:\n",
    "                    print(f\"\"\"Loss increased after epoch {e+1}, final loss was {self.best_loss:.3f}, using the model from epoch {e+1-eval_every}\"\"\")\n",
    "                    self.net = last_model\n",
    "                    # ensure self.optim is still updating self.net\n",
    "                    setattr(self.optim, 'net', self.net)\n",
    "                    break\n",
    "            \n",
    "            #call this at the end of each epoch\n",
    "            if self.optim.final_lr:  #<------added\n",
    "                self.optim._decay_lr()   #<-----added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.323\n",
      "Validation loss after 20 epochs is 0.306\n",
      "Loss increased after epoch 30, final loss was 0.306, using the model from epoch 20\n",
      "Accuracy:  0.9336\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89, \n",
    "                  activation=Tanh()),\n",
    "            Dense(neurons=10, \n",
    "                  activation=Linear())],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20200720)\n",
    "\n",
    "trainer = Trainer(model, SGDMomentum(lr=0.2, momentum=0.9,\n",
    "                                    final_lr=0.05, decay_type='exponential'))\n",
    "trainer.fit(X_train, y_train_encode, X_test, y_test_encode,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20200720,\n",
    "            batch_size=60)\n",
    "\n",
    "calc_accuracy(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
