{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythondsai",
   "display_name": "pythonDSAI"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "Load the MNIST data, and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing). Then train various classifiers, such as a Random Forest classifier, a Logistic Regression classifier, an SVM, and a MLPClassifier (I haven't taught you yet, but its a simple neural network with multi-layers). \n",
    " \n",
    "Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?\n",
    " \n",
    "Last, attemp to use XGBoost.  Does it improve the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "#implement Bagging ensembles of Decision Trees from scratch\n",
    "\n",
    "\"\"\"\n",
    "Coding considerations:\n",
    "1. Load any data you would like to apply\n",
    "\n",
    "2. Make sure to subsample *with* replacement.  I.e., \n",
    "   we should allow the same training instance to be subsampled \n",
    "   for the same predictor.  For example, X[5] can appear multiple\n",
    "   times in the nth predictor (it may not even appear at all!)\n",
    "\n",
    "3. Once all predictors are trained, the bagging ensemble can make a prediction.\n",
    "   By simply aggregating, commonly using mode for classification and\n",
    "   average for regression\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. #implement AdaBoost for binary classification from scratch\n",
    "\n",
    "\"\"\"\n",
    "Coding considerations:\n",
    "\n",
    "1. Load any binary classification problem you would like to apply\n",
    "\n",
    "2. Given binary classification, AdaBoost expects input as -1 and 1\n",
    "\n",
    "3. Once all predictors have been trained, the final predictions are\n",
    "simply the np.sign of a_j * y_pred.  This works because if more\n",
    "classifiers say that it is positive, then the sum will turn out\n",
    "to be positive, thus we can simply look at the np.sign to know\n",
    "the class\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\n",
    "\n",
    "#implement Gradient Boosting from scratch\n",
    "#To add to the challenge, we shall create our algorithm that\n",
    "#works for both binary classification and regression\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Coding considerations:\n",
    "\n",
    "1. Load one binary classification and one regression problem you would like to apply\n",
    "\n",
    "2. To make our algorithm work for both regression and binary classification\n",
    "   We can perform this via a params called 'loss'\n",
    "   if loss is defined as \"mse\", then its a regression problem and\n",
    "   the residual errors shall be simply calculated as y - f(x)\n",
    "   if loss is defined as \"logistic\", then its a classification problem\n",
    "   and the residual errors shall be calculated as y - sigmoid(x)\n",
    "   the reason of using sigmoid is obvious, since we want to create\n",
    "   a mapping of x to something between 0 and 1 so it can be compared\n",
    "   to y, thus we use sigmoid\n",
    "\n",
    "3. You may wonder what classifier or regressor to use for this\n",
    "   problem.  Luckily, if we define the loss function with sigmoid,\n",
    "   we can simply use regressor as our estimator, since we already\n",
    "   define sigmoid function that map any continuous value to values \n",
    "   between 0 and 1.  The only thing you have to do is to \n",
    "   create a tolerance function mapping >0.5 to 1 otherwise 0\n",
    "\n",
    "4. When finding the loss, it is important to calculate the loss\n",
    "   based on the total errors made by all models you have trained\n",
    "   up to nth iteration\n",
    "   \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ]
}