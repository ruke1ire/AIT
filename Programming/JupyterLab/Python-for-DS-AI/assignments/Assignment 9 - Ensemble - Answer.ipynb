{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythondsai",
   "display_name": "pythonDSAI"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 9 - Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "Load the MNIST data, and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing). Then train various classifiers, such as a Random Forest classifier, a Logistic Regression classifier, an SVM, and a MLPClassifier (I haven't taught you yet, but its a simple neural network with multi-layers). \n",
    " \n",
    "Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?\n",
    " \n",
    "Last, attemp to use XGBoost.  Does it improve the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "#fetching\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "\n",
    "#make sure is int\n",
    "mnist.target = mnist.target.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Load the MNIST data and split it into a training set, a validation set, and \n",
    "#a test set (e.g., use 50,000 instances for training, 10,000 for \n",
    "#validation, and 10,000 for testing)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    mnist.data, mnist.target, test_size=10000, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42)\n",
    "mlp_clf = MLPClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [rnd_clf, log_clf, svm_clf, mlp_clf]\n",
    "for model in models:\n",
    "    print(\"Training the\", model.__class__.__name__)\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[model.score(X_val, y_val) for model in models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like Logistic Regression is hurting performance.  Not sure whether we should include it on the ensemble but we will think it later....Let's first define a Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "named_estimators = [\n",
    "    (\"rnd_clf\", rnd_clf),\n",
    "    (\"log_clf\", log_clf),\n",
    "    (\"svm_clf\", svm_clf),\n",
    "    (\"mlp_clf\", mlp_clf),\n",
    "]\n",
    "\n",
    "voting_clf = VotingClassifier(named_estimators)\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's print out the individual estimator\n",
    "[estimator.score(X_val, y_val) for estimator in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we can see that Logistic Regression is hurting the ensemble performance.  We can easily remove using del or fit again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del voting_clf.estimators_[1]  #second estimator\n",
    "\n",
    "#print estimators to make sure its deleted\n",
    "voting_clf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the score again\n",
    "voting_clf.score(X_val, y_val)  #yay increase a little!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, voting classifier is hard voting.  Let's try soft voting.  We can either fit again, but we can also easily set params to soft (this works because soft/hard is simply a function of all the models we already fitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.voting = \"soft\"\n",
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...seems like hard wins....\n",
    "\n",
    "Finally, let's try on our testing set which we should never touch it until the final testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.voting = \"hard\"\n",
    "print(\"Voting classifier score:\", voting_clf.score(X_test, y_test))\n",
    "print(\"Each classifier scor: \")\n",
    "[estimator.score(X_test, y_test) for estimator in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...it seems that the voting classifier only slighly increase the accuracy by little, when compared to the best estimator\n",
    "\n",
    "let's try XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_clf = xgboost.XGBClassifier() \n",
    "\n",
    "#not improved after 2 iterations\n",
    "xgb_clf.fit(X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "print(\"Score: \", xgb_clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "#implement Bagging ensembles of Decision Trees from scratch\n",
    "\n",
    "\"\"\"\n",
    "Coding considerations:\n",
    "1. Load any data you would like to apply\n",
    "\n",
    "2. Make sure to subsample *with* replacement.  I.e., \n",
    "   we should allow the same training instance to be subsampled \n",
    "   for the same predictor.  For example, X[5] can appear multiple\n",
    "   times in the nth predictor (it may not even appear at all!)\n",
    "\n",
    "3. Once all predictors are trained, the bagging ensemble can make a prediction.\n",
    "   By simply aggregating, commonly using mode for classification and\n",
    "   average for regression\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                test_size=0.3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "n_estimators = 5\n",
    "boostrap_ratio = 0.1\n",
    "tree_params = {'max_depth': 2, 'criterion':'gini', 'min_samples_split': 5}\n",
    "models = [DecisionTreeClassifier(**tree_params) for _ in range(n_estimators)]\n",
    "\n",
    "#sample size for each predictor\n",
    "sample_size = int(boostrap_ratio * len(X_train))\n",
    "\n",
    "#2 for x and y\n",
    "xsamples = []\n",
    "ysamples = []\n",
    "\n",
    "#subsamples for each model\n",
    "for _ in range(n_estimators):\n",
    "    xsample = []\n",
    "    ysample = []\n",
    "    ##sampling with replacement; i.e., sample can occur more than once\n",
    "    #for the same predictor\n",
    "    for _ in range(sample_size):\n",
    "        idx = random.randrange(X_train.shape[0])\n",
    "        xsample.append(X_train[idx])\n",
    "        ysample.append(y_train[idx])\n",
    "    xsamples.append(xsample)\n",
    "    ysamples.append(ysample)\n",
    "\n",
    "#convert to numpy for easier manipulation\n",
    "#usually if we start with list.append and later convert to np\n",
    "#is faster than using np.empty\n",
    "xsamples = np.asarray(xsamples)\n",
    "ysamples = np.asarray(ysamples)\n",
    "\n",
    "#fitting each estimator\n",
    "for i, model in enumerate(models):\n",
    "    _X = xsamples[i, :]\n",
    "    _y = ysamples[i, :]\n",
    "    model.fit(_X, _y)\n",
    "    \n",
    "#make prediction and return the probabilities\n",
    "predictions = []\n",
    "for model in models:\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions.append(y_pred)\n",
    "    \n",
    "#we can use unpacking technique to remove the outerlist\n",
    "#we need [0] because .mode return both list and the counts\n",
    "[y_pred] = stats.mode(predictions)[0]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. #implement AdaBoost for binary classification from scratch\n",
    "\n",
    "\"\"\"\n",
    "Coding considerations:\n",
    "\n",
    "1. Load any binary classification problem you would like to apply\n",
    "\n",
    "2. Given binary classification, AdaBoost expects input as -1 and 1\n",
    "\n",
    "3. Once all predictors have been trained, the final predictions are\n",
    "simply the np.sign of a_j * y_pred.  This works because if more\n",
    "classifiers say that it is positive, then the sum will turn out\n",
    "to be positive, thus we can simply look at the np.sign to know\n",
    "the class\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=500)\n",
    "y = np.where(y==0,-1,1)  #change our y to be -1 if it is 0, otherwise 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_samples = X_train.shape[0]\n",
    "n_estimators = 20\n",
    "tree_params = {'max_depth': 1}\n",
    "models = [DecisionTreeClassifier(**tree_params) for _ in range(n_estimators)]\n",
    "\n",
    "#initially, we set our weight to 1/n\n",
    "W = np.ones(shape=n_samples) / n_samples\n",
    "\n",
    "#keep collection of a_j\n",
    "a_js = np.zeros(shape=n_estimators)\n",
    "\n",
    "for j, model in enumerate(models):\n",
    "    \n",
    "    #train weak learner\n",
    "    model.fit(X_train, y_train, sample_weight=W)\n",
    "    \n",
    "    #compute the errors r_j\n",
    "    y_pred = model.predict(X_train) \n",
    "    compare = y_pred != y_train  #this works since True * w = 1 * w\n",
    "#     compare = np.array([int(x) for x in (y_pred != y_train)])\n",
    "    \n",
    "    r_j = np.sum(W * compare) / sum(W)\n",
    "    \n",
    "    #compute the predictor weight a_j\n",
    "    #let eta to be 1 as adaboost default \n",
    "    #if predictor is doing well, a_j will be big\n",
    "    eta = 1\n",
    "    a_j = eta * np.log ((1 - r_j) / r_j)\n",
    "    a_js[j] = a_j\n",
    "    \n",
    "    #update sample weight; divide sum of W to normalize\n",
    "    W = (W * np.exp(a_j)) / sum (W)\n",
    "    \n",
    "        \n",
    "#make weighted predictions\n",
    "Hx = 0\n",
    "for i, model in enumerate(models):\n",
    "    y_pred = model.predict(X_test)\n",
    "    Hx += a_js[i] * y_pred\n",
    "    \n",
    "y_pred = np.sign(Hx)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\n",
    "\n",
    "#implement Gradient Boosting from scratch\n",
    "#To add to the challenge, we shall create our algorithm that\n",
    "#works for both binary classification and regression\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Coding considerations:\n",
    "\n",
    "1. Load one binary classification and one regression problem you would like to apply\n",
    "\n",
    "2. To make our algorithm work for both regression and binary classification\n",
    "   We can perform this via a params called 'loss'\n",
    "   if loss is defined as \"mse\", then its a regression problem and\n",
    "   the residual errors shall be simply calculated as y - f(x)\n",
    "   if loss is defined as \"logistic\", then its a classification problem\n",
    "   and the residual errors shall be calculated as y - sigmoid(x)\n",
    "   the reason of using sigmoid is obvious, since we want to create\n",
    "   a mapping of x to something between 0 and 1 so it can be compared\n",
    "   to y, thus we use sigmoid\n",
    "\n",
    "3. You may wonder what classifier or regressor to use for this\n",
    "   problem.  Luckily, if we define the loss function with sigmoid,\n",
    "   we can simply use regressor as our estimator, since we already\n",
    "   define sigmoid function that map any continuous value to values \n",
    "   between 0 and 1.  The only thing you have to do is to \n",
    "   create a tolerance function mapping >0.5 to 1 otherwise 0\n",
    "\n",
    "4. When finding the loss, it is important to calculate the loss\n",
    "   based on the total errors made by all models you have trained\n",
    "   up to nth iteration\n",
    "   \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "def mse(y, f):\n",
    "    return y - f\n",
    "\n",
    "def logit(y, f):\n",
    "    return y - expit(f)\n",
    "\n",
    "def fit(X, y, models, loss=\"mse\"):\n",
    "    \n",
    "    if loss == \"mse\":\n",
    "        loss_func = mse\n",
    "    else:\n",
    "        loss_func = logit\n",
    "    \n",
    "    models_trained = []\n",
    "    \n",
    "    #using DummyRegressor is a good technique for starting model\n",
    "    first_model = DummyRegressor(strategy='constant', constant=0)\n",
    "    first_model.fit(X, y)\n",
    "    models_trained.append(first_model)\n",
    "    \n",
    "    #fit the estimators\n",
    "    for i, model in enumerate(models):\n",
    "        #predict using all the weak learners we trained up to\n",
    "        #this point\n",
    "        y_pred = predict(X, models_trained)\n",
    "        \n",
    "        #errors will be the total errors maded by models_trained\n",
    "        residual = loss_func(y, y_pred)\n",
    "        \n",
    "        #fit the next model with residual\n",
    "        model.fit(X, residual)\n",
    "        \n",
    "        models_trained.append(model)\n",
    "        \n",
    "    return models_trained\n",
    "        \n",
    "def predict(X, models):\n",
    "    learning_rate = 0.1  ##hard code for now\n",
    "    f0 = models[0].predict(X)  #first use the dummy model\n",
    "    boosting = sum(learning_rate * model.predict(X) for model in models[1:])\n",
    "    return f0 + boosting\n",
    "\n",
    "def change_to_0_1(sample):\n",
    "    return int(sample > 0.5)\n",
    "\n",
    "def predict_class(X, models):\n",
    "    probas = expit(predict(X, models))\n",
    "    return np.array([change_to_0_1(proba) for proba in probas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "n_estimators = 200\n",
    "tree_params = {'max_depth': 1}\n",
    "models = [DecisionTreeRegressor(**tree_params) for _ in range(n_estimators)]\n",
    "\n",
    "#fit the models\n",
    "models = fit(X_train, y_train, models)\n",
    "\n",
    "#predict\n",
    "y_pred = predict(X_test, models)\n",
    "\n",
    "#print metrics\n",
    "print(\"Custom MSE: \", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "#Compare to sklearn: ls is the same as our mse\n",
    "sklearn_model = GradientBoostingRegressor(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate = 0.1,\n",
    "    max_depth=1,\n",
    "    loss='ls'\n",
    ")\n",
    "\n",
    "y_pred_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#print metrics\n",
    "print(\"Sklearn MSE: \", mean_squared_error(y_test, y_pred_sk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "n_estimators = 200\n",
    "tree_params = {'max_depth': 1}\n",
    "models = [DecisionTreeRegressor(**tree_params) for _ in range(n_estimators)]\n",
    "\n",
    "#fit the models\n",
    "models = fit(X_train, y_train, models, loss=\"logistic\")\n",
    "\n",
    "#predict\n",
    "y_pred = predict_class(X_test, models)\n",
    "\n",
    "# #print metrics\n",
    "print(\"Custom accuracy: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Compare to sklearn: ls is the same as our mse\n",
    "sklearn_model = GradientBoostingClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate = 0.1,\n",
    "    max_depth=1\n",
    ")\n",
    "\n",
    "y_pred_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#print metrics\n",
    "print(\"Sklearn accuracy: \", accuracy_score(y_test, y_pred_sk))\n",
    "\n"
   ]
  }
 ]
}