{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP - Midterm - 2020\n",
    "\n",
    "## Instruction\n",
    "\n",
    "- Modify this file to be Midterm-<Your FirstName-[First Letter of Last Name]>, e.g., <code>Midterm-Chaklam-S.ipynb</code>\n",
    "- This exam accounts for 25% of the overall course assessment.\n",
    "- This exam is open-booked; open-internet.\n",
    "- You ARE NOT allowed to use sklearn or any libraries, unless stated.\n",
    "- The completed exams shall be submitted at the Google Classroom\n",
    "- All code should be **complemented with comments**, unless it's really obvious.  **I and Joe reserve the privilege to give you zero for any part of the question where the benefit of doubt is not justified**\n",
    "\n",
    "## Examination Rules:\n",
    "- For **offline** students, you may leave the room temporarily with the approval and supervision of the proctors. No extra time will be added to the exam in such cases.\n",
    "- For **online** students, you are required to turn on your webcam during the entire period of the exam time\n",
    "- Students will be allowed to leave at the **earliest 45 minutes** after the exam has started\n",
    "- **All work should belong to you**.  A student should NOT engage in the following activities which proctors reserve the right to interpret any of such act as academic dishonesty without questioning:\n",
    "    - Chatting with any human beings physically or via online methods\n",
    "    - Plagiarism of any sort, i.e., copying from internet sources or friends.  **Both copee and copier shall be given a minimum penalty of zero mark for that particular question or the whole exam.**\n",
    "- No make-up exams are allowed.  Special considerations may be given upon a valid reason on unpredictable events such as accidents or serious sickness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (21 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1). **The rabbit**: (5pts)\n",
    "\n",
    "Once upon a time, there is a father rabbit lives in a far away jungle. Everyday, the father rabbit has to go out and find some carrots for his family. In his family there are mother rabbit, grampa rabbit, sister rabbit, and his son. In total there are 5 rabbits to feed. In one day, the adult rabbits (himself, mother rabbit and sister rabbit) will eat 3 carrots while the elderly eat 2 carrots and baby rabbit eat 1 carrot.\n",
    "\n",
    "Unfortunately, the carrots are not easy to find. The father rabbit has to travel into the scary jungle and find some carrot then bring them back to the family before the sunset at 6PM.\n",
    "\n",
    "- Every 1 km, the rabbit will find 3 carrots.\n",
    "- The rabbit will use 1 hour to travel 1 km.\n",
    "\n",
    "In summary, in order to find the least number of carrot for each day, the rabbit will have to use (3 + 3 + 3 + 2 + 1)/3 =  4 hours. This mean that he has to leave the house at the latest 10AM (4 hours for go out and another 4 for comming back).\n",
    "\n",
    "This daily work has to be done exactly on time, leaving to late will cause whether his life or his family life. Would you like to help the rabbit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 'yes' to help the rabbit or 'no' to refuse the challenge. (if yes -> 1 pt)\n",
    "\n",
    "#Father Rabbit 3 carrots\n",
    "#Mother Rabbit 3 carrots\n",
    "#Grandpa Rabbit 2 carrots\n",
    "#Sister Rabbit 3 carrots\n",
    "#Son Rabbit 1 carrots\n",
    "\n",
    "#Leaves house at the latest 10AM\n",
    "#yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good to hear that young programmer!!\n",
    "\n",
    "What I have in mind is to build a clock that when the rabbit puts the number of (adult, elderly, young) rabbit, it will calculate how many hours is required for a travel. Of cause we have to make it as a function because the number of each rabbit type will change over the time.\n",
    "\n",
    "- Write a function <code>carrot</code> that takes three integers as an input in follow this format <code>(adult, elderly, young)</code> (1pt)\n",
    "- The function will calculate number of hour required for travelling a day. (1pt)\n",
    "- The function will also calcualte the time to leave. Think of it as an alarm clock for leaving the house (1pt)\n",
    "- The function will return a tuple (\\#hours, #time) (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "def carrot(adult, elderly, young):\n",
    "    hours = (adult*3+elderly*2+young)/3 \n",
    "    leave = 18-2*hours\n",
    "#     if(hours < 12):\n",
    "#         print(leave)\n",
    "#         hour_str = str(int(leave))+\":\"+str(int(60*(leave%1)))+\"AM\"\n",
    "#     else:\n",
    "#         hour_str = str(int(leave-12))+\":\"+str(int(60*(leave%1)))+\"PM\"\n",
    "    return (hours,leave)\n",
    "\n",
    "carrot(3,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2). **Print the shape**: (16pts)\n",
    "\n",
    "- Write a function <code>square</code> that takes integer as an input. (1pt)\n",
    "- The function will return a string of * in the shape of a square with both width and height equal to the input interger. (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Level: 3\n",
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "Level: 5\n",
    "*****\n",
    "*****\n",
    "*****\n",
    "*****\n",
    "*****\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def square(n):\n",
    "    #Create two loops to print \"*\"\n",
    "    for j in range(n):\n",
    "        for i in range(n):\n",
    "            print(\"*\",end=\"\")\n",
    "        print()\n",
    "\n",
    "square(5)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function <code>triangle</code> that takes integer as an input. (1pt)\n",
    "- The function will return a string of * in the shape of a triangle with level equal to the input integer. (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Level: 3\n",
    "  *\n",
    " **\n",
    "***\n",
    "\n",
    "Level: 5\n",
    "    *\n",
    "   **\n",
    "  ***\n",
    " ****\n",
    "*****\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "def triangle(n):\n",
    "    #Create a loop for each row, each row will have its corresponding number of spaces and number of \"*\"\n",
    "    for i in range(n):\n",
    "        spaces = \" \"*(n-i-1)\n",
    "        ast = \"*\"*(i+1)\n",
    "        print(spaces,end=\"\")\n",
    "        print(ast)\n",
    "\n",
    "\n",
    "triangle(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function <code>pyramid</code> that takes integer as an input. (1pt)\n",
    "- The function will return the string of * in the shape of a pyramid with level equal to the input interger. (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Level: 3 \n",
    "  * \n",
    " ***\n",
    "*****\n",
    "\n",
    "Level: 5\n",
    "    *\n",
    "   ***\n",
    "  *****\n",
    " *******\n",
    "*********\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "def pyramid(n):\n",
    "    #similar to the previous question, each row will have their respective amount of spaces and asterisks\n",
    "    for i in range(n):\n",
    "        spaces = \" \"*(n-i-1)\n",
    "        ast = \"*\"*(i*2+1)\n",
    "        print(spaces,end=\"\")\n",
    "        print(ast,end=\"\")\n",
    "        print(spaces)\n",
    "\n",
    "pyramid(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's combine the three algorithms into one single class.\n",
    "- Create a class named <code>MyShape</code> that can do the followings\n",
    "    - Take two arguments during the class construction. The first one is an integer and the second one is a string. The names are <code>level</code> and <code>shape</code> (1pt)\n",
    "    - Check the input arguments whether the interger is in the range of \\[1,10\\] and string is in the set of {'squ','tri','pyr'}. Raise a <code>ValueError</code>. (2pts)\n",
    "    - Both attributes should be able to change via a set method **only**. <code>set\\[attrName\\]</code> (1pt)\n",
    "    - Of cause, the set method should check the out of range too. (1pt)\n",
    "    - To check the current setting, write a get method. <code>get\\[attrName\\]</code> (1pt)\n",
    "    - Print the shape with method <code>show</code>. It should return the string of the current shape with the correct level (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example 1\n",
    "\n",
    ">>> ms = MyShape(2,'tri')\n",
    ">>> ms.show()\n",
    " *\n",
    "**\n",
    ">>> ms.setLevel(3)\n",
    ">>> ms.setShape('squ')\n",
    ">>> ms.show()\n",
    "***\n",
    "***\n",
    "***\n",
    ">>> ms.setShape('a')\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in <module>\n",
    "ValueError: ..........\n",
    ">>>\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "class MyShape():\n",
    "    def __init__(self,level,shape):\n",
    "        self.l = level\n",
    "        self.s = shape\n",
    "        self.checkLevel(self.l)\n",
    "        self.checkShape(self.s)\n",
    "        \n",
    "    def checkLevel(self,level):\n",
    "        if(level > 10 or level < 1):\n",
    "            raise ValueError(\"Invalid Level\")\n",
    "        \n",
    "    def checkShape(self,shape):\n",
    "        if(shape not in [\"squ\",\"tri\",\"pyr\"]):\n",
    "            raise ValueError(\"Invalid Shape\")\n",
    "    \n",
    "    def setLevel(self,level):\n",
    "        self.checkLevel(level)\n",
    "        self.l = level\n",
    "    \n",
    "    def setShape(self,shape):\n",
    "        self.checkShape(shape)\n",
    "        self.s = shape\n",
    "    \n",
    "    def getLevel(self):\n",
    "        return self.l\n",
    "    \n",
    "    def getShape(self):\n",
    "        return self.s\n",
    "    \n",
    "    def pyramid(self,n):\n",
    "        for i in range(n):\n",
    "            spaces = \" \"*(n-i-1)\n",
    "            ast = \"*\"*(i*2+1)\n",
    "            print(spaces,end=\"\")\n",
    "            print(ast,end=\"\")\n",
    "            print(spaces)\n",
    "            \n",
    "    def triangle(self,n):\n",
    "        for i in range(n):\n",
    "            spaces = \" \"*(n-i-1)\n",
    "            ast = \"*\"*(i+1)\n",
    "            print(spaces,end=\"\")\n",
    "            print(ast)\n",
    "            \n",
    "    def square(self,n):\n",
    "        for j in range(n):\n",
    "            for i in range(n):\n",
    "                print(\"*\",end=\"\")\n",
    "            print()\n",
    "    \n",
    "    def show(self):\n",
    "        if(self.s == 'squ'):\n",
    "            self.square(self.l)\n",
    "        elif(self.s == 'tri'):\n",
    "            self.triangle(self.l)\n",
    "        elif(self.s == 'pyr'):\n",
    "            self.pyramid(self.l)\n",
    "        else:\n",
    "            print(\"HI\")\n",
    "            \n",
    "\n",
    "ms = MyShape(3,'pyr')\n",
    "print(ms.getShape())\n",
    "print(ms.getLevel())\n",
    "ms.show()\n",
    "ms.setShape('squ')\n",
    "ms.setLevel(10)\n",
    "print(ms.getShape())\n",
    "print(ms.getLevel())\n",
    "ms.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2). **ML Skill**\n",
    "$$ y = ax + b $$\n",
    "The above equation is your favorite linear equation where <code>a,b</code> are the constant value indicate the slope and the offset of the line in the graph.\n",
    "\n",
    "We all know given and two points. $$ (x_1,y_1) (x_2,y_2) $$ you can find <code>a,b</code> very easy using Geometry \n",
    "\n",
    "$$ a = \\frac{y_2 - y_1}{x_2 - x_1} $$\n",
    "$$ b = y_i - ax_i$$\n",
    "\n",
    "Since we have learnt that using LinearRegression can find the value of the <code>a,b</code> too.\n",
    "\n",
    "Now, do the followings.\n",
    "\n",
    "- Write a function <code>drawLine</code> that takes two tuples as inputs.\n",
    "- Calculate <code>a,b</code> using Geometry.\n",
    "- Draw the first graph with scatter on the given two points and a line.\n",
    "- Calculate <code>a,b</code> using LinearRegression with Batch Gradient Descent.\n",
    "    - Generate 1000 sample data along the line.\n",
    "    - Regress on the data using LinearRegression-BatchGradientDescent\n",
    "- Draw the second graph with scatter on the given two points and a line.\n",
    "- Does both method yeild the same outcome? Which method runs faster? (use timeit)\n",
    "- What will happen if the data is normalize first? (draw another graph and timeit)\n",
    "- What will happen if the data is standardize first? (draw another graph and timeit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegressionModel:\n",
    "#1. hypothesis function\n",
    "    def h(self, X, theta):      \n",
    "        hypothesis = X@theta\n",
    "        return hypothesis\n",
    "        \n",
    "#2. cost function\n",
    "    def cost(self, X, y, theta, average = False):\n",
    "        #expects X to be a design matrix, y to be a column vector and theta to be a column vector\n",
    "        if(average == False):\n",
    "            J = 1/2*(self.h(X,theta)-y).T@(self.h(X,theta)-y)\n",
    "        else:\n",
    "            J = 1/(2*X.shape[0])*(self.h(X,theta)-y).T@(self.h(X,theta)-y)\n",
    "        return J\n",
    "\n",
    "#3. gradient function\n",
    "    def gradient(self, X, y, theta, average = False):\n",
    "        if(average == False):\n",
    "            dJ = X.T@(self.h(X,theta)-y)\n",
    "        else:\n",
    "            dJ = X.T@(self.h(X,theta)-y)/(X.shape[0])\n",
    "        return dJ\n",
    "    \n",
    "#4. batch gradient descent\n",
    "    def batch_gd(self, X, y, initial_theta, max_iteration, alpha, tolerance = 0,average = False):\n",
    "        cost = []\n",
    "        theta = initial_theta\n",
    "        iteration = 0\n",
    "        cost.append(self.cost(X,y,theta,average))\n",
    "        for n in range(max_iteration):\n",
    "            gradient = self.gradient(X,y,theta,average)\n",
    "            theta = theta - alpha*gradient\n",
    "            cost.append(self.cost(X,y,theta,average))\n",
    "            iteration += 1\n",
    "            if(self.mean_squared_error(X,y,theta) < tolerance):\n",
    "                return theta,cost,iteration\n",
    "        cost = np.array(cost)\n",
    "        return theta,cost,iteration\n",
    "    \n",
    "#5. normal equation\n",
    "    def normal_equation(self, X, y):\n",
    "        theta = np.linalg.inv(X.T@X)@X.T@y\n",
    "        return theta\n",
    "            \n",
    "#5. predict\n",
    "    def predict(self,X,theta):\n",
    "        prediction = self.h(X,theta)\n",
    "        return prediction\n",
    "    \n",
    "#6. score/error calculation\n",
    "    def mean_squared_error(self,X,y,theta):\n",
    "        mse = self.cost(X,y,theta,average = True)*2\n",
    "        return mse\n",
    "        \n",
    "#7. plotting cost\n",
    "    def plot_cost(self,cost, iteration_no):\n",
    "        iteration_series = np.arange(0,iteration_no+1)\n",
    "        ax = plt.axes()\n",
    "        ax.plot(iteration_series, cost)\n",
    "\n",
    "def drawLine(one,two):\n",
    "    # drawLine function as stated in the problem\n",
    "    x,y = zip(one,two)\n",
    "    print(\"timeit for using the equations\")\n",
    "    #using timeit, we can see how long it takes to perform theese operations\n",
    "    a = (y[1]-y[0])/(x[1]-x[0])\n",
    "    b = y[0]-a*x[0]\n",
    "    %timeit (y[1]-y[0])/(x[1]-x[0]), y[0]-a*x[0]\n",
    "    \n",
    "    #Plot for equation\n",
    "    fig,ax = plt.subplots(2,2,figsize = (20,20))\n",
    "    ax[0,0].scatter(x,y)\n",
    "    ax[0,0].plot(x,y,'r')\n",
    "    \n",
    "    x_sample = np.linspace(np.min(x),np.max(x),1000)\n",
    "    y_sample = x_sample*a+b\n",
    "    \n",
    "    LR = LinearRegressionModel()\n",
    "    iterations = 1000\n",
    "    alpha = 0.001\n",
    "    initial_theta = np.zeros(2)\n",
    "    x_sample_inserted = np.insert(x_sample[:,np.newaxis],0,1,axis=1)\n",
    "    \n",
    "    #using batch gradient descent on data which hasn't been standardized or normalized\n",
    "    print(\"timeit for batch gradient descent\")\n",
    "    %timeit LR.batch_gd(x_sample_inserted,y_sample,initial_theta,iterations,alpha)\n",
    "    theta,cost,iteration = LR.batch_gd(x_sample_inserted,y_sample,initial_theta,iterations,alpha,tolerance=1e-7)\n",
    "    y_pred = LR.predict(x_sample_inserted,theta)\n",
    "    #LR.plot_cost(cost,iteration)\n",
    "    print(\"iteration:\",iteration)\n",
    "    \n",
    "    MSE = LR.mean_squared_error(x_sample_inserted,y_sample,theta)\n",
    "    print(\"MSE =\",MSE)\n",
    "\n",
    "    ax[0,1].scatter(x,y)\n",
    "    ax[0,1].plot(x,y,'r')\n",
    "    ax[0,1].scatter(x_sample,y_pred)\n",
    "    \n",
    "    mini = np.min(x_sample)\n",
    "    maxi = np.max(x_sample)\n",
    "    x_norm = (x_sample-mini)/(maxi-mini)\n",
    "\n",
    "    x_norm_inserted = np.insert(x_norm[:,np.newaxis],0,1,axis=1)\n",
    "    \n",
    "    iterations2 = 1000\n",
    "    alpha2 = 0.001\n",
    "    initial_theta2 = np.zeros(2)\n",
    "    \n",
    "    print(\"timeit for normalized data\")\n",
    "    %timeit LR.batch_gd(x_norm_inserted,y_sample,initial_theta2,iterations2,alpha2)\n",
    "    theta2,cost2,iteration2 = LR.batch_gd(x_norm_inserted,y_sample,initial_theta2,iterations2,alpha2,tolerance=1e-7)\n",
    "    y_pred2 = LR.predict(x_norm_inserted,theta2)\n",
    "    print(\"iteration2\",iteration2)\n",
    "    MSE2 = LR.mean_squared_error(x_norm_inserted,y_sample,theta2)\n",
    "    print(\"MSE =\",MSE2)\n",
    "    \n",
    "    ax[1,0].scatter(x,y)\n",
    "    ax[1,0].plot(x,y,'r')\n",
    "    ax[1,0].scatter(x_norm*(maxi-mini)+mini,y_pred2)\n",
    "    \n",
    "    mean = np.mean(x_sample)\n",
    "    std = np.std(x_sample)\n",
    "    x_stan = (x_sample-mean)/std\n",
    "    \n",
    "    x_stan_inserted = np.insert(x_stan[:,np.newaxis],0,1,axis=1)\n",
    "\n",
    "    iterations3= 1000\n",
    "    alpha3 = 0.001\n",
    "    initial_theta3 = np.zeros(2)\n",
    "    \n",
    "    print(\"timeit for standardized data\")\n",
    "    %timeit LR.batch_gd(x_stan_inserted,y_sample,initial_theta3,iterations3,alpha3,average=True)\n",
    "    theta3,cost3,iteration3 = LR.batch_gd(x_stan_inserted,y_sample,initial_theta3,iterations3,alpha3,tolerance=1e-7)\n",
    "    y_pred3 = LR.predict(x_stan_inserted,theta3)\n",
    "    print(\"iteration3\",iteration3)\n",
    "    MSE3 = LR.mean_squared_error(x_stan_inserted,y_sample,theta3)\n",
    "    print(\"MSE =\",MSE3)\n",
    "    \n",
    "    ax[1,1].scatter(x,y)\n",
    "    ax[1,1].plot(x,y,'r')\n",
    "    ax[1,1].scatter(x_stan*std+mean,y_pred3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"slope =\",a,\"intercept =\",b)\n",
    "\n",
    "drawLine((1,2),(-0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (69 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1). **Exploratory Data Analysis**:\n",
    "- Load the data <code>\"howlongwelive.csv\"</code> to pandas and print the first 5 and last 5 rows of data  (1 or 0pt)\n",
    "\n",
    "- Print the shape, feature names, and summary (describe) of the data (1 or 0pt)\n",
    "\n",
    "- Check whether there is missing data. (1 or 0pt)\n",
    "\n",
    "- Fix all missing data using means or mode (1 or 0pt)  \n",
    "\n",
    "- Since Hepatatis B has a lot of nans, and highly correlate with Diptheria, simply drop column Hepatatis.  Also drop column Population since there are way too many nans (1 or 0pt)\n",
    "\n",
    "- If there are any features which are string and you want to use them as features, we need to convert them to int or float.  For now, convert <code>Status</code> to 0 or 1 (1 or 0pt)\n",
    "\n",
    "- Rename column <code>thinness_1-19_years</code> to <code>thinness_10-19_years</code> (1 or 0pt)\n",
    "\n",
    "- Perform a <code>groupby</code> country and plot their life expectancy.  Which country has the lowest/highest life expectancy? (1 or 0pt)\n",
    "\n",
    "- Plot average life expectancy of developed country vs. developing country.  (1 or 0pt)\n",
    "\n",
    "- Perform a t-test of life expectancy between developed and developing countries.  Is the result significant? (1 or 0pt)\n",
    "\n",
    "- Perform a <code>pairplot</code> to see which features are likely to have strong predictive power for life expectancy.  Identify the most 3 important features.  (1 or 0pt)\n",
    "\n",
    "- Perform a histogram of life expectancy.  Is it normal? (1 or 0pt)\n",
    "\n",
    "2). **Regression**\n",
    "- Prepare your <code>X</code> and <code>y</code> into Numpy array (you have to map from Pandas to numpy).  For X, prepare two versions of them.  For first <code>X_selected</code>, you have to choose the most 3 important features from above, and for second <code>X_all</code>, simply use all features (you may want to omit Country since they are categorical). Set <code>y</code> to life expectancy. (1 or 0pt)\n",
    "\n",
    "- Perform standardization using Numpy way (NOT sklearn way). (1 or 0pt)\n",
    "\n",
    "- Perform train-test split by using Numpy way (NOT sklearn way).  Use test size of 0.3.   (1 or 0pt)\n",
    "\n",
    "- Perform assertion whether your splitting is correct accordingly (1 or 0pt)\n",
    "\n",
    "- Write a class <code>Regression(X, y, grad_method, max_iter, alpha, tol, decay, decay_iter, decay_rate, stop_delay_counter, verbose, lam, poly, poly_deg)</code> that can perform the followings:\n",
    "    - Mini-batch, Stochastic, and Batch Gradient Descent (each 2pts)\n",
    "    - Polynomial of degree k (2 or 0pt)\n",
    "    - Decay learning rate (1 or 0pt)\n",
    "        - Decay learning rate is a learning rate that becomes smaller after certian iteration. For example, after 5 iterations, the learning rate will reduce to 95% of the current learning rate.\n",
    "        - To implement it, simply multiply current learning rate with some constant <code>decay_rate</code>. For now, set it to 0.9\n",
    "    - Regularization with ridge (2 or 0pt)\n",
    "    - Must have at least four methods for <code>fit()</code> (i.e., for finding weights) <code>predict()</code> (i.e., for predicting X_test data), <code>score()</code> (i.e., for returning $r^2$ score), and <code>mse()</code> (return mse) (each 1pt)\n",
    "    - Accepts <code>X</code>, <code>y</code>, <code>grad_method</code> (default set to \"batch\"), <code>alpha</code> (learning rate), <code>max_iter</code>, <code>tol</code>, <code>decay</code> (whether to use decay learning rate; default set to False), <code>decay_iter</code> (after how many iterations will the decay apply), <code>stop_delay_counter</code> (this is the maximum number of times that decay the learning rate), <code>verbose</code> (default is set to False, whether model will display the Cost for each iteration), <code>lam</code> (this is the ridge regularization parameter), <code>poly</code> (default is set to False), and <code>poly_deg</code> (default is set to 2) (each 1/13pt)\n",
    "\n",
    "- Create the following 3 models **from your class** (For any unspecified parameters, feel free to use any :D)\n",
    "    1. For the first model, transform your feature using polynomial degree 3, then perform linear regression with batch gradient descent with early stopping of <code>tol</code> 1e-3  (1 or 0pt)\n",
    "    2. For the second model, perform linear regression with mini-batch gradient desent with early stopping of <code>tol</code> 1e-3 (1 or 0pt)\n",
    "    3. For the third model, perform ridge regression with stochastic gradient desent with early stopping of <code>tol</code> 1e-3 and <code>decay</code> set to True and <code>lam</code> to 1e-4 (1 or 0pt)\n",
    "    \n",
    "- Create Lasso model from Sklearn with default parameters (1 or 0pt)\n",
    "\n",
    "- For these four models, using two different versions of X, perform a cross validation of 10 folds, comparing the four models * two versions of X.  Here you should implement cross validation. Report which one is the best candidate model (3pts for implement from scratch or 1pt for using sklearn)  \n",
    "    - Recall that in a 10 folds cross validation, you split your data into 10 even pieces.  Then you run 10 iterations, where in each iteration, you pick 1 of this piece as the validation set, and the rest as training set.  Once you reach the 10th iteration, you would have already exhaust all the 10 pieces as validation set.\n",
    "\n",
    "- Using the best model, fit again with the training data.  Plot the weights using bar charts along the feature names.  Before you actually plot the weights, we need to multiply these weights by their feature standard deviation, so to reduce these weights to same unit of measure.  Interpret these weights and what they imply.  (For those who are curious why we need to multiply with std, you may read this > https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#interpreting-coefficients-scale-matters  (2 or 0pt)   \n",
    "\n",
    "- Perform predictions on testing data.  Print adjusted $r^2$ and mse. (1 or 0pt)\n",
    "\n",
    "- Plot the predicted values against actual values (1 or 0pt)\n",
    "\n",
    "3). **Classification**\n",
    "\n",
    "- Change your y to discrete value.  Here split y into three class, {0, 1, 2}, where 0 belongs to low life expectancy group, and 2 for the high life expectancy group. (1 or 0pt)\n",
    "\n",
    "- Write a class for multinomial logistic regression with stochastic gradient descent. Must have at least six methods for <code>fit()</code> (i.e., for finding weights) <code>predict()</code> (i.e., for predicting X_test data), <code>accuracy()</code> (i.e., for returning accuracy score), <code>recall()</code>, <code>precision()</code>, and <code>f1()</code> (each 1pt)\n",
    "\n",
    "- Using the best X_train of the two suggested by the cross validation step, fit the data with your class.  (1 or 0pt)\n",
    "\n",
    "- Perform predictions on testing data.  Print accuracy, recall, precision, and f1_score from your class. (1 or 0pt)\n",
    "\n",
    "- Plot the decision boundary with the X_test data.  To plot this, you may want to choose only 2 features.  (1 or 0pt)\n",
    "\n",
    "4). **Final verdict**\n",
    "\n",
    "- Attempt to do whatever ways - including sklearn or scratch - or change your features, or do feature enginnering such that your mse is lowest possible.  (0 to 5pts - following class normal distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1). **Exploratory Data Analysis**:\n",
    "- Load the data <code>\"howlongwelive.csv\"</code> to pandas and print the first 5 and last 5 rows of data  (1 or 0pt)\n",
    "\n",
    "- Print the shape, feature names, and summary (describe) of the data (1 or 0pt)\n",
    "\n",
    "- Check whether there is missing data. (1 or 0pt)\n",
    "\n",
    "- Fix all missing data using means or mode (1 or 0pt)  \n",
    "\n",
    "- Since Hepatatis B has a lot of nans, and highly correlate with Diptheria, simply drop column Hepatatis.  Also drop column Population since there are way too many nans (1 or 0pt)\n",
    "\n",
    "- If there are any features which are string and you want to use them as features, we need to convert them to int or float.  For now, convert <code>Status</code> to 0 or 1 (1 or 0pt)\n",
    "\n",
    "- Rename column <code>thinness_1-19_years</code> to <code>thinness_10-19_years</code> (1 or 0pt)\n",
    "\n",
    "- Perform a <code>groupby</code> country and plot their life expectancy.  Which country has the lowest/highest life expectancy? (1 or 0pt)\n",
    "\n",
    "- Plot average life expectancy of developed country vs. developing country.  (1 or 0pt)\n",
    "\n",
    "- Perform a t-test of life expectancy between developed and developing countries.  Is the result significant? (1 or 0pt)\n",
    "\n",
    "- Perform a <code>pairplot</code> to see which features are likely to have strong predictive power for life expectancy.  Identify the most 3 important features.  (1 or 0pt)\n",
    "\n",
    "- Perform a histogram of life expectancy.  Is it normal? (1 or 0pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"howlongwelive.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape:\",data.shape)\n",
    "print(\"features:\",data.columns)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"amount of missing data for each column:\")\n",
    "print(np.sum(data.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(data.mean(),inplace=True)\n",
    "print(\"No more missing data:\")\n",
    "print(np.sum(data.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns = [\"Hepatitis B\",\"Population\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, levels = pd.factorize(data[\"Status\"])\n",
    "data[\"Status\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the name of the columns has to be found by using data.columns first \n",
    "data.rename(columns={\" thinness  1-19 years\":\"thinness_10-19_years\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data.groupby('Country')\n",
    "fig = plt.figure(figsize=(30,10))\n",
    "ax = plt.axes()\n",
    "grouped.mean()['Life expectancy '].plot(ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(grouped.mean()['Life expectancy '])\n",
    "series = grouped.mean()['Life expectancy ']\n",
    "series.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data.groupby(\"Status\")\n",
    "grouped.mean()['Life expectancy '].plot()\n",
    "print(levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "data.groupby(\"Status\").apply(lambda df: stats.ttest_ind(df['Life expectancy '], df['Status']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(data,x_vars=[\"Life expectancy \"],y_vars = col)\n",
    "#adult mortality, schoolilng, income composition seem to be the most correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Life expectancy '].hist()\n",
    "#its a skewed graph so it's not normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:,[\"Adult Mortality\",\"Income composition of resources\",\"Schooling\"]].values.astype(float)\n",
    "X_all = data.drop(columns=['Country','Life expectancy ']).values.astype(float)\n",
    "y = data['Life expectancy '].values.astype(float)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_all.shape)\n",
    "\n",
    "mean = np.mean(X,axis=0)\n",
    "std = np.std(X,axis=0)\n",
    "X_norm = (X-mean)/std\n",
    "\n",
    "mean_all = np.mean(X_all,axis=0)\n",
    "std_all = np.std(X_all,axis=0)\n",
    "X_all_norm = (X_all-mean_all)/std_all\n",
    "\n",
    "ix = np.arange(X.shape[0])\n",
    "np.random.shuffle(ix)\n",
    "m = X.shape[0]\n",
    "percentage = 0.7\n",
    "ix_train = ix[:int(m*percentage)]\n",
    "ix_test = ix[int(m*percentage):]\n",
    "\n",
    "X_norm_train = X_norm[ix_train]\n",
    "X_all_norm_train = X_all_norm[ix_train]\n",
    "X_norm_test = X_norm[ix_test]\n",
    "X_all_norm_test = X_all_norm[ix_test]\n",
    "y_train = y[ix_train]\n",
    "y_test = y[ix_test]\n",
    "\n",
    "print(X_norm_test.shape[0]/(X_norm_train.shape[0]+X_norm_test.shape[0]))\n",
    "\n",
    "assert X_norm_test.shape[0]/(X_norm_train.shape[0]+X_norm_test.shape[0]) < 0.31 and X_norm_test.shape[0]/(X_norm_train.shape[0]+X_norm_test.shape[0]) > 0.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression():\n",
    "    def __init__(self,X,y,grad_method,max_iter,alpha,tol,decay,decay_iter,decay_rate,stop_delay_counter,verbose,lam,poly,poly_deg):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.grad_method = grad_method\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "        self.decay = decay\n",
    "        self.decay_iter = decay_iter\n",
    "        self.decay_rate = decay_rate\n",
    "        self.stop_delay_counter = stop_delay_counter\n",
    "        self.verbose = verbose\n",
    "        self.lam = lam\n",
    "        self.poly = poly\n",
    "        self.poly_deg = poly_deg\n",
    "        \n",
    "    #1. hypothesis function\n",
    "    def h(self, theta):      \n",
    "        hypothesis = self.X@theta\n",
    "        return hypothesis\n",
    "        \n",
    "#2. cost function\n",
    "    def cost(self, theta):\n",
    "        J = 1/(2*X.shape[0])*(self.h(X,theta)-y).T@(self.h(X,theta)-y)\n",
    "        return J\n",
    "    \n",
    "    def cost_reg(self, X, y, theta,lamb):\n",
    "        J = 1/(2*X.shape[0])*(self.h(X,theta)-y).T@(self.h(X,theta)-y) + lamb*np.sum(theta@theta)\n",
    "        return J\n",
    "\n",
    "#3. gradient function\n",
    "    def gradient_reg(self, X, y, theta, average = False):\n",
    "        dJ = X.T@(self.h(X,theta)-y)/(X.shape[0]) + theta*lamb\n",
    "        return dJ\n",
    "    \n",
    "    def gradient(self, X, y, theta, average = False):\n",
    "        dJ = X.T@(self.h(X,theta)-y)/(X.shape[0])\n",
    "        return dJ\n",
    "        \n",
    "    def mini_batch():\n",
    "        cost = []\n",
    "        theta = initial_theta\n",
    "        iteration = 0\n",
    "        cost.append(self.cost(X,y,theta,average))\n",
    "        ix = np.arange(y.size)\n",
    "        percentage = 0.1\n",
    "        ix_range = int(percentage*ix.size)\n",
    "        for n in range(max_iteration):\n",
    "            np.random.shuffle(ix)\n",
    "            X_batch = X[:ix_range]\n",
    "            y_batch = X[:ix_range]\n",
    "            gradient = self.gradient(X_batch,y_batch,theta,average)\n",
    "            theta = theta - alpha*gradient\n",
    "            cost.append(self.cost(X_batch,y_batch,theta,average))\n",
    "            iteration += 1\n",
    "        cost = np.array(cost)\n",
    "        return theta,cost,iteration\n",
    "    \n",
    "    def stochastic():\n",
    "        cost = []\n",
    "        theta = initial_theta\n",
    "        iteration = 0\n",
    "        cost.append(self.cost(X,y,theta,average))\n",
    "        for n in range(max_iteration):\n",
    "            for i in X.shape[0]:\n",
    "                if(self.grad_method == 'ridge'):\n",
    "                    gradient = self.gradient_reg(X[i],y,theta,average)\n",
    "                else:\n",
    "                    gradient = self.gradient(X[i],y,theta,average)\n",
    "                theta = theta - alpha*gradient\n",
    "                cost.append(self.cost(X[i],y,theta,average))\n",
    "                iteration += 1\n",
    "        cost = np.array(cost)\n",
    "        return theta,cost,iteration\n",
    "    \n",
    "    def batch():\n",
    "        cost = []\n",
    "        theta = initial_theta\n",
    "        iteration = 0\n",
    "        cost.append(self.cost(X,y,theta,average))\n",
    "        for n in range(max_iteration):\n",
    "            gradient = self.gradient(X,y,theta,average)\n",
    "            theta = theta - alpha*gradient\n",
    "            cost.append(self.cost(X,y,theta,average))\n",
    "            iteration += 1\n",
    "        cost = np.array(cost)\n",
    "        return theta,cost,iteration\n",
    "    \n",
    "    def polynomial_features():\n",
    "        pass\n",
    "    \n",
    "    def decay_learning_rate():\n",
    "        pass\n",
    "    \n",
    "    def ridge():\n",
    "        pass\n",
    "    \n",
    "    def fit():\n",
    "        pass\n",
    "    \n",
    "    def predict():\n",
    "        pass\n",
    "    \n",
    "    def score():\n",
    "        pass\n",
    "    \n",
    "    def mse():\n",
    "        pass\n",
    "    \n",
    "#would've been better if the class wasn't fixed to the form Regression(X, y, grad_method, max_iter, alpha, tol, decay, decay_iter, decay_rate, stop_delay_counter, verbose, lam, poly, poly_deg)\n",
    "#because i have written a class but not in this way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2). **Regression**\n",
    "- Prepare your <code>X</code> and <code>y</code> into Numpy array (you have to map from Pandas to numpy).  For X, prepare two versions of them.  For first <code>X_selected</code>, you have to choose the most 3 important features from above, and for second <code>X_all</code>, simply use all features (you may want to omit Country since they are categorical). Set <code>y</code> to life expectancy. (1 or 0pt)\n",
    "\n",
    "- Perform standardization using Numpy way (NOT sklearn way). (1 or 0pt)\n",
    "\n",
    "- Perform train-test split by using Numpy way (NOT sklearn way).  Use test size of 0.3.   (1 or 0pt)\n",
    "\n",
    "- Perform assertion whether your splitting is correct accordingly (1 or 0pt)\n",
    "\n",
    "- Write a class <code>Regression(X, y, grad_method, max_iter, alpha, tol, decay, decay_iter, decay_rate, stop_delay_counter, verbose, lam, poly, poly_deg)</code> that can perform the followings:\n",
    "    - Mini-batch, Stochastic, and Batch Gradient Descent (each 2pts)\n",
    "    - Polynomial of degree k (2 or 0pt)\n",
    "    - Decay learning rate (1 or 0pt)\n",
    "        - Decay learning rate is a learning rate that becomes smaller after certian iteration. For example, after 5 iterations, the learning rate will reduce to 95% of the current learning rate.\n",
    "        - To implement it, simply multiply current learning rate with some constant <code>decay_rate</code>. For now, set it to 0.9\n",
    "    - Regularization with ridge (2 or 0pt)\n",
    "    - Must have at least four methods for <code>fit()</code> (i.e., for finding weights) <code>predict()</code> (i.e., for predicting X_test data), <code>score()</code> (i.e., for returning $r^2$ score), and <code>mse()</code> (return mse) (each 1pt)\n",
    "    - Accepts <code>X</code>, <code>y</code>, <code>grad_method</code> (default set to \"batch\"), <code>alpha</code> (learning rate), <code>max_iter</code>, <code>tol</code>, <code>decay</code> (whether to use decay learning rate; default set to False), <code>decay_iter</code> (after how many iterations will the decay apply), <code>stop_delay_counter</code> (this is the maximum number of times that decay the learning rate), <code>verbose</code> (default is set to False, whether model will display the Cost for each iteration), <code>lam</code> (this is the ridge regularization parameter), <code>poly</code> (default is set to False), and <code>poly_deg</code> (default is set to 2) (each 1/13pt)\n",
    "\n",
    "- Create the following 3 models **from your class** (For any unspecified parameters, feel free to use any :D)\n",
    "    1. For the first model, transform your feature using polynomial degree 3, then perform linear regression with batch gradient descent with early stopping of <code>tol</code> 1e-3  (1 or 0pt)\n",
    "    2. For the second model, perform linear regression with mini-batch gradient desent with early stopping of <code>tol</code> 1e-3 (1 or 0pt)\n",
    "    3. For the third model, perform ridge regression with stochastic gradient desent with early stopping of <code>tol</code> 1e-3 and <code>decay</code> set to True and <code>lam</code> to 1e-4 (1 or 0pt)\n",
    "    \n",
    "- Create Lasso model from Sklearn with default parameters (1 or 0pt)\n",
    "\n",
    "- For these four models, using two different versions of X, perform a cross validation of 10 folds, comparing the four models * two versions of X.  Here you should implement cross validation. Report which one is the best candidate model (3pts for implement from scratch or 1pt for using sklearn)  \n",
    "    - Recall that in a 10 folds cross validation, you split your data into 10 even pieces.  Then you run 10 iterations, where in each iteration, you pick 1 of this piece as the validation set, and the rest as training set.  Once you reach the 10th iteration, you would have already exhaust all the 10 pieces as validation set.\n",
    "\n",
    "- Using the best model, fit again with the training data.  Plot the weights using bar charts along the feature names.  Before you actually plot the weights, we need to multiply these weights by their feature standard deviation, so to reduce these weights to same unit of measure.  Interpret these weights and what they imply.  (For those who are curious why we need to multiply with std, you may read this > https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#interpreting-coefficients-scale-matters  (2 or 0pt)   \n",
    "\n",
    "- Perform predictions on testing data.  Print adjusted $r^2$ and mse. (1 or 0pt)\n",
    "\n",
    "- Plot the predicted values against actual values (1 or 0pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3). **Classification**\n",
    "\n",
    "- Change your y to discrete value.  Here split y into three class, {0, 1, 2}, where 0 belongs to low life expectancy group, and 2 for the high life expectancy group. (1 or 0pt)\n",
    "\n",
    "- Write a class for multinomial logistic regression with stochastic gradient descent. Must have at least six methods for <code>fit()</code> (i.e., for finding weights) <code>predict()</code> (i.e., for predicting X_test data), <code>accuracy()</code> (i.e., for returning accuracy score), <code>recall()</code>, <code>precision()</code>, and <code>f1()</code> (each 1pt)\n",
    "\n",
    "- Using the best X_train of the two suggested by the cross validation step, fit the data with your class.  (1 or 0pt)\n",
    "\n",
    "- Perform predictions on testing data.  Print accuracy, recall, precision, and f1_score from your class. (1 or 0pt)\n",
    "\n",
    "- Plot the decision boundary with the X_test data.  To plot this, you may want to choose only 2 features.  (1 or 0pt)\n",
    "\n",
    "4). **Final verdict**\n",
    "\n",
    "- Attempt to do whatever ways - including sklearn or scratch - or change your features, or do feature enginnering such that your mse is lowest possible.  (0 to 5pts - following class normal distributions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
